import React, { createContext, useContext, useReducer, useEffect, useState } from 'react'
import { modelSettingsService } from '../services/ai/modelSettingsService'
import { modelSyncService } from '../services/ai/modelSyncService'
import { getRecommendedModels, allLatestModels, type LatestModelInfo } from '../services/ai/latestModelsData'
// AIServiceManager í´ë¼ì´ì–¸íŠ¸ì‚¬ì´ë“œ ì œê±° - ì„œë²„ì‚¬ì´ë“œ API ì‚¬ìš©

// AI ëª¨ë¸ íƒ€ì… ì •ì˜
export interface AIModel {
  id: string
  name: string
  provider: 'openai' | 'anthropic' | 'google' | 'custom'
  model_id: string
  cost_per_input_token: number
  cost_per_output_token: number
  status: string
  capabilities: string[]
  max_tokens: number
  available: boolean
  metadata?: Record<string, any>
}

// ìƒíƒœ íƒ€ì… ì •ì˜
interface AIModelState {
  selectedProviderId: string | null  // 1ì°¨ ì„ íƒ: í”„ë¡œë°”ì´ë”
  selectedModelId: string | null     // 2ì°¨ ì„ íƒ: ì„¸ë¶€ ëª¨ë¸
  availableModels: AIModel[]
  loading: boolean
  error: string | null
  lastSyncTime: string | null
  syncInProgress: boolean
}

// ì•¡ì…˜ íƒ€ì… ì •ì˜
type AIModelAction =
  | { type: 'SET_LOADING'; payload: boolean }
  | { type: 'SET_ERROR'; payload: string | null }
  | { type: 'SET_MODELS'; payload: AIModel[] }
  | { type: 'SELECT_PROVIDER'; payload: string }
  | { type: 'SELECT_MODEL'; payload: string }
  | { type: 'CLEAR_SELECTION' }
  | { type: 'SET_SYNC_STATUS'; payload: { syncInProgress: boolean; lastSyncTime?: string } }

// ì´ˆê¸° ìƒíƒœ
const initialState: AIModelState = {
  selectedProviderId: null,
  selectedModelId: null,
  availableModels: [],
  loading: false,
  error: null,
  lastSyncTime: null,
  syncInProgress: false
}

// localStorage í‚¤
const STORAGE_KEYS = {
  PROVIDER: 'ai_selected_provider',
  MODEL: 'ai_selected_model'
}

// ë¦¬ë“€ì„œ
function aiModelReducer(state: AIModelState, action: AIModelAction): AIModelState {
  switch (action.type) {
    case 'SET_LOADING':
      return { ...state, loading: action.payload }
    case 'SET_ERROR':
      return { ...state, error: action.payload, loading: false }
    case 'SET_MODELS':
      return { ...state, availableModels: action.payload, loading: false }
    case 'SELECT_PROVIDER':
      // localStorageì— í”„ë¡œë°”ì´ë” ì €ì¥
      localStorage.setItem(STORAGE_KEYS.PROVIDER, action.payload)
      localStorage.removeItem(STORAGE_KEYS.MODEL) // í”„ë¡œë°”ì´ë” ë³€ê²½ ì‹œ ëª¨ë¸ ì´ˆê¸°í™”
      return {
        ...state,
        selectedProviderId: action.payload,
        selectedModelId: null // í”„ë¡œë°”ì´ë” ë³€ê²½ ì‹œ ëª¨ë¸ ì„ íƒ ì´ˆê¸°í™”
      }
    case 'SELECT_MODEL':
      // localStorageì— ëª¨ë¸ ì €ì¥
      localStorage.setItem(STORAGE_KEYS.MODEL, action.payload)
      return { ...state, selectedModelId: action.payload }
    case 'CLEAR_SELECTION':
      // localStorageì—ì„œ ì œê±°
      localStorage.removeItem(STORAGE_KEYS.PROVIDER)
      localStorage.removeItem(STORAGE_KEYS.MODEL)
      return { ...state, selectedProviderId: null, selectedModelId: null }
    case 'SET_SYNC_STATUS':
      return {
        ...state,
        syncInProgress: action.payload.syncInProgress,
        lastSyncTime: action.payload.lastSyncTime || state.lastSyncTime
      }
    default:
      return state
  }
}

// ì»¨í…ìŠ¤íŠ¸ íƒ€ì… ì •ì˜
interface AIModelContextType {
  state: AIModelState
  selectProvider: (providerId: string) => Promise<void>
  selectModel: (modelId: string) => Promise<void>
  clearSelection: () => void
  refreshModels: () => Promise<void>
  syncModels: () => Promise<void>
  getSelectedModel: () => AIModel | null
  getProviderModels: (providerId: string) => AIModel[]
  getAvailableProviders: () => string[]
  getRecommendedModels: () => {
    fastest: AIModel | null
    cheapest: AIModel | null
    best_performance: AIModel | null
    balanced: AIModel | null
  }
  getModelStatistics: () => Promise<any>
  isSyncing: boolean
}

// ì»¨í…ìŠ¤íŠ¸ ìƒì„±
const AIModelContext = createContext<AIModelContextType | undefined>(undefined)

// í”„ë¡œë°”ì´ë” ì»´í¬ë„ŒíŠ¸
export function AIModelProvider({ children }: { children: React.ReactNode }) {
  const [state, dispatch] = useReducer(aiModelReducer, initialState)
  const [isSyncing, setIsSyncing] = useState(false)

  // ìµœì‹  ëª¨ë¸ ë°ì´í„°ë¥¼ AIModel í˜•ì‹ìœ¼ë¡œ ë³€í™˜
  const convertLatestModelToAIModel = (latestModel: LatestModelInfo): AIModel => ({
    id: latestModel.id,
    name: latestModel.name,
    provider: latestModel.provider,
    model_id: latestModel.model_id,
    cost_per_input_token: latestModel.cost_per_input_token,
    cost_per_output_token: latestModel.cost_per_output_token,
    status: latestModel.status,
    capabilities: latestModel.capabilities,
    max_tokens: latestModel.max_tokens,
    available: latestModel.status === 'active',
    metadata: latestModel.metadata
  })

  // ì´ˆê¸° ëª¨ë¸ ì„ íƒ í•¨ìˆ˜ (localStorage ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸)
  const setInitialModel = async (models: AIModel[]) => {
    try {
      // 1. localStorageì—ì„œ ì €ì¥ëœ ì„ íƒ í™•ì¸
      const savedProvider = localStorage.getItem(STORAGE_KEYS.PROVIDER)
      const savedModel = localStorage.getItem(STORAGE_KEYS.MODEL)

      console.log('ğŸ” ì €ì¥ëœ ëª¨ë¸ í™•ì¸:', { savedProvider, savedModel })

      // 2. ì €ì¥ëœ ëª¨ë¸ì´ ìˆê³  ìœ íš¨í•˜ë©´ ë³µì›
      if (savedProvider && savedModel) {
        const savedModelData = models.find(m => m.id === savedModel && m.provider === savedProvider)

        if (savedModelData && savedModelData.available) {
          console.log('âœ… ì €ì¥ëœ ëª¨ë¸ ë³µì›:', savedModelData.name, '(' + savedModelData.model_id + ')')
          dispatch({ type: 'SELECT_PROVIDER', payload: savedModelData.provider })
          dispatch({ type: 'SELECT_MODEL', payload: savedModelData.id })
          return
        } else {
          console.warn('âš ï¸ ì €ì¥ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.')
          // ìœ íš¨í•˜ì§€ ì•Šì€ ì €ì¥ê°’ ì œê±°
          localStorage.removeItem(STORAGE_KEYS.PROVIDER)
          localStorage.removeItem(STORAGE_KEYS.MODEL)
        }
      }

      // 3. ì €ì¥ëœ ëª¨ë¸ì´ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ ì„ íƒ
      // 1ìˆœìœ„: Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)
      let defaultModel = models.find(m => m.model_id === 'claude-sonnet-4-5-20250929')

      // 2ìˆœìœ„: Claude 3 Sonnet (claude-3-5-sonnet-20241022)
      if (!defaultModel) {
        defaultModel = models.find(m => m.model_id === 'claude-3-5-sonnet-20241022')
      }

      // 3ìˆœìœ„: ì²« ë²ˆì§¸ Anthropic ëª¨ë¸
      if (!defaultModel) {
        defaultModel = models.find(m => m.provider === 'anthropic' && m.available)
      }

      // 4ìˆœìœ„: ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ëª¨ë¸
      if (!defaultModel) {
        defaultModel = models.find(m => m.available)
      }

      if (defaultModel) {
        console.log('ğŸ¯ ê¸°ë³¸ ëª¨ë¸ ì„ íƒ:', defaultModel.name, '(' + defaultModel.model_id + ')')
        dispatch({ type: 'SELECT_PROVIDER', payload: defaultModel.provider })
        dispatch({ type: 'SELECT_MODEL', payload: defaultModel.id })

        // ì„œë²„ì‚¬ì´ë“œ APIë¥¼ í†µí•œ AI ëª¨ë¸ ì„ íƒ ì™„ë£Œ
        console.log('âœ… AI ëª¨ë¸ ì„ íƒ ì™„ë£Œ (ì„œë²„ì‚¬ì´ë“œ API ì‚¬ìš©):', defaultModel.name)
      } else {
        console.warn('âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ë³¸ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
      }
    } catch (error) {
      console.error('ì´ˆê¸° ëª¨ë¸ ì„ íƒ ì¤‘ ì˜¤ë¥˜:', error)
    }
  }

  // AI ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ (ìµœì‹  ëª¨ë¸ ë°ì´í„° ìš°ì„  ì ìš©)
  const loadModels = async () => {
    try {
      dispatch({ type: 'SET_LOADING', payload: true })
      dispatch({ type: 'SET_ERROR', payload: null })

      console.log('ğŸ”„ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...')

      // 1. ìµœì‹  ëª¨ë¸ ë°ì´í„°ë¥¼ ìš°ì„  ë¡œë“œ
      const latestModelsConverted: AIModel[] = allLatestModels.map(convertLatestModelToAIModel)
      console.log('ğŸ“Š ìµœì‹  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ:', latestModelsConverted.length, 'ê°œ')

      // 2. ê¸°ì¡´ ë¡œì»¬ ëª¨ë¸ë„ ë¡œë“œ (ë°±ê·¸ë¼ìš´ë“œ) - AIë§¤ë‹ˆì € ëª¨ë¸ì€ ì„œë²„ì‚¬ì´ë“œë¡œ ì´ë™
      try {
        const localModels = await modelSettingsService.getActiveModels().catch(() => [])

        // ë¡œì»¬ ëª¨ë¸ í¬ë§·íŒ…
        const formattedLocalModels: AIModel[] = localModels.map(model => ({
          id: model.id,
          name: model.name,
          provider: model.provider as 'openai' | 'anthropic' | 'google' | 'custom',
          model_id: model.model_id,
          cost_per_input_token: model.cost_per_input_token,
          cost_per_output_token: model.cost_per_output_token,
          status: model.status || 'active',
          capabilities: model.capabilities || [],
          max_tokens: model.max_tokens,
          available: model.status === 'active',
          metadata: model.metadata
        }))

        // 3. ëª¨ë“  ëª¨ë¸ ë³‘í•© (ìµœì‹  ëª¨ë¸ ìš°ì„ , ì¤‘ë³µ ì œê±°)
        const allModels = [...latestModelsConverted]

        // ë¡œì»¬ ëª¨ë¸ ì¶”ê°€ (ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë§Œ)
        formattedLocalModels.forEach(localModel => {
          const exists = allModels.find(model => model.model_id === localModel.model_id)
          if (!exists) {
            allModels.push(localModel)
          }
        })

        console.log('âœ… ì „ì²´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ:', allModels.length, 'ê°œ')
        dispatch({ type: 'SET_MODELS', payload: allModels })

        // 4. ì´ˆê¸° ëª¨ë¸ ì„ íƒ (localStorage ë³µì› ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸)
        await setInitialModel(allModels)
      } catch (error) {
        console.warn('âš ï¸ ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ìµœì‹  ëª¨ë¸ë§Œ ì‚¬ìš©:', error)
        // ë¡œì»¬ ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í•´ë„ ìµœì‹  ëª¨ë¸ì€ í‘œì‹œ
        dispatch({ type: 'SET_MODELS', payload: latestModelsConverted })

        // ì´ˆê¸° ëª¨ë¸ ì„ íƒ (localStorage ë³µì› ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸)
        await setInitialModel(latestModelsConverted)
      }

      // ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì—…ë°ì´íŠ¸
      const lastSync = modelSyncService.getLastSyncTime()
      if (lastSync) {
        dispatch({ type: 'SET_SYNC_STATUS', payload: { syncInProgress: false, lastSyncTime: lastSync } })
      }
    } catch (error) {
      console.error('Failed to load AI models:', error)
      dispatch({ type: 'SET_ERROR', payload: 'AI ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.' })
    }
  }

  // setupAIServiceManager í•¨ìˆ˜ ì œê±°ë¨ - ì„œë²„ì‚¬ì´ë“œ API ì‚¬ìš©ìœ¼ë¡œ ë¶ˆí•„ìš”

  // AI ëª¨ë¸ ë™ê¸°í™” í•¨ìˆ˜
  const syncModels = async () => {
    try {
      setIsSyncing(true)
      dispatch({ type: 'SET_SYNC_STATUS', payload: { syncInProgress: true } })
      dispatch({ type: 'SET_ERROR', payload: null })

      console.log('ğŸ”„ AI ëª¨ë¸ ë™ê¸°í™” ì‹œì‘...')
      const result = await modelSyncService.syncAllModels()

      if (result.success) {
        console.log('âœ… AI ëª¨ë¸ ë™ê¸°í™” ì™„ë£Œ:', result.summary)
        // ë™ê¸°í™” í›„ ëª¨ë¸ ëª©ë¡ ìƒˆë¡œê³ ì¹¨
        await loadModels()
      } else {
        console.error('âŒ AI ëª¨ë¸ ë™ê¸°í™” ì‹¤íŒ¨:', result.details.errors)
        dispatch({ type: 'SET_ERROR', payload: `ë™ê¸°í™” ì‹¤íŒ¨: ${result.summary.errors}ê°œ ì˜¤ë¥˜ ë°œìƒ` })
      }

      dispatch({ type: 'SET_SYNC_STATUS', payload: {
        syncInProgress: false,
        lastSyncTime: new Date().toISOString()
      } })
    } catch (error) {
      console.error('AI ëª¨ë¸ ë™ê¸°í™” ì¤‘ ì˜¤ë¥˜:', error)
      dispatch({ type: 'SET_ERROR', payload: 'AI ëª¨ë¸ ë™ê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.' })
      dispatch({ type: 'SET_SYNC_STATUS', payload: { syncInProgress: false } })
    } finally {
      setIsSyncing(false)
    }
  }

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ ëª¨ë¸ ë¡œë”©
  useEffect(() => {
    loadModels()
  }, [])

  // ì»¨í…ìŠ¤íŠ¸ ê°’
  const contextValue: AIModelContextType = {
    state,
    selectProvider: async (providerId: string) => {
      dispatch({ type: 'SELECT_PROVIDER', payload: providerId })

      // ì„œë²„ì‚¬ì´ë“œ APIë¥¼ í†µí•œ í”„ë¡œë°”ì´ë” ì„ íƒ ì™„ë£Œ
      const selectedModel = state.availableModels.find(m => m.provider === providerId)
      if (selectedModel) {
        console.log('âœ… í”„ë¡œë°”ì´ë” ì„ íƒ ì™„ë£Œ (ì„œë²„ì‚¬ì´ë“œ API ì‚¬ìš©):', selectedModel.provider)
      }
    },
    selectModel: async (modelId: string) => {
      dispatch({ type: 'SELECT_MODEL', payload: modelId })

      // ì„œë²„ì‚¬ì´ë“œ APIë¥¼ í†µí•œ ëª¨ë¸ ì„ íƒ ì™„ë£Œ
      const selectedModel = state.availableModels.find(m => m.id === modelId)
      if (selectedModel) {
        console.log('âœ… ëª¨ë¸ ì„ íƒ ì™„ë£Œ (ì„œë²„ì‚¬ì´ë“œ API ì‚¬ìš©):', selectedModel.name)
      }
    },
    clearSelection: () => {
      dispatch({ type: 'CLEAR_SELECTION' })
    },
    refreshModels: loadModels,
    syncModels: syncModels,
    getSelectedModel: () => {
      return state.availableModels.find(model => model.id === state.selectedModelId) || null
    },
    getProviderModels: (providerId: string) => {
      return state.availableModels.filter(model => model.provider === providerId)
    },
    getAvailableProviders: () => {
      const providers = [...new Set(state.availableModels.map(model => model.provider))]
      return providers
    },
    getRecommendedModels: () => {
      const recommended = getRecommendedModels()
      return {
        fastest: state.availableModels.find(m => m.model_id === recommended.fastest.model_id) || null,
        cheapest: state.availableModels.find(m => m.model_id === recommended.cheapest.model_id) || null,
        best_performance: state.availableModels.find(m => m.model_id === recommended.best_performance.model_id) || null,
        balanced: state.availableModels.find(m => m.model_id === recommended.balanced.model_id) || null
      }
    },
    getModelStatistics: async () => {
      return await modelSyncService.getModelStatistics()
    },
    isSyncing: isSyncing
  }

  return (
    <AIModelContext.Provider value={contextValue}>
      {children}
    </AIModelContext.Provider>
  )
}

// ì»¤ìŠ¤í…€ í›…
export function useAIModel() {
  const context = useContext(AIModelContext)
  if (context === undefined) {
    throw new Error('useAIModel must be used within an AIModelProvider')
  }
  return context
}

// ì„ íƒëœ ëª¨ë¸ë§Œ ê°€ì ¸ì˜¤ëŠ” í›…
export function useSelectedAIModel() {
  const { state, getSelectedModel } = useAIModel()
  return {
    selectedModel: getSelectedModel(),
    isLoading: state.loading,
    error: state.error
  }
}

// ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ë§Œ ê°€ì ¸ì˜¤ëŠ” í›…
export function useAvailableAIModels() {
  const { state } = useAIModel()
  return {
    models: state.availableModels.filter(model => model.available),
    allModels: state.availableModels,
    isLoading: state.loading,
    error: state.error
  }
}