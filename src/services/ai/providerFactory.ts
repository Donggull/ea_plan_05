import { ApiUsageService } from '../apiUsageService'
import { RateLimiter } from '../../lib/rateLimiter'

// ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ë“¤ì„ staticìœ¼ë¡œ ì‚¬ìš©

// AI ì œê³µì—…ì²´ íƒ€ì… ì •ì˜
export type AIProvider = 'openai' | 'anthropic' | 'google' | 'custom'

// AI ìš”ì²­ ë©”ì‹œì§€ íƒ€ì…
export interface AIMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

// AI ëª¨ë¸ ì„¤ì • ì¸í„°í˜ì´ìŠ¤
export interface AIModelConfig {
  id: string
  name: string
  provider: AIProvider
  model_id: string
  api_key?: string
  api_endpoint?: string
  max_tokens: number
  temperature?: number
  top_p?: number
  cost_per_input_token: number
  cost_per_output_token: number
  rate_limits: {
    requests_per_minute: number
    tokens_per_minute: number
  }
}

// AI ì‘ë‹µ ì¸í„°í˜ì´ìŠ¤
export interface AIResponse {
  content: string
  model: string
  usage: {
    input_tokens: number
    output_tokens: number
    total_tokens: number
  }
  cost: number
  response_time: number
  finish_reason: 'stop' | 'length' | 'error'
}

// AI ìš”ì²­ ì˜µì…˜
export interface AIRequestOptions {
  messages: AIMessage[]
  max_tokens?: number
  temperature?: number
  top_p?: number
  stream?: boolean
  user_id?: string
}

// í´ë°± ì„¤ì •
export interface FallbackConfig {
  enabled: boolean
  models: string[] // ìš°ì„ ìˆœìœ„ëŒ€ë¡œ ì •ë ¬ëœ ëª¨ë¸ ID ëª©ë¡
  max_retries: number
  retry_delay: number // ms
}

// ì—ëŸ¬ íƒ€ì…
export class AIProviderError extends Error {
  constructor(
    message: string,
    public provider: AIProvider,
    public model: string,
    public statusCode?: number,
    public retryable: boolean = false
  ) {
    super(message)
    this.name = 'AIProviderError'
  }
}

// ê¸°ë³¸ AI ì œê³µì—…ì²´ í´ë˜ìŠ¤
abstract class BaseAIProvider {
  protected config: AIModelConfig

  constructor(config: AIModelConfig) {
    this.config = config
  }

  abstract generateCompletion(options: AIRequestOptions): Promise<AIResponse>

  // ê³µí†µ í† í° ê³„ì‚° ë©”ì„œë“œ
  protected estimateTokens(text: string): number {
    // ê°„ë‹¨í•œ í† í° ì¶”ì • - ì‹¤ì œë¡œëŠ” ëª¨ë¸ë³„ í† í¬ë‚˜ì´ì € ì‚¬ìš©
    return Math.ceil(text.length / 4)
  }

  // ë¹„ìš© ê³„ì‚°
  protected calculateCost(inputTokens: number, outputTokens: number): number {
    return (inputTokens * this.config.cost_per_input_token) +
           (outputTokens * this.config.cost_per_output_token)
  }

  // Rate Limiting ì²´í¬
  protected async checkRateLimit(userId: string): Promise<void> {
    const allowed = await RateLimiter.checkRateLimit(userId, 'user', 1)
    if (!allowed.allowed) {
      throw new AIProviderError(
        `Rate limit exceeded: ${allowed.reason}`,
        this.config.provider,
        this.config.model_id,
        429,
        true
      )
    }
  }
}

// OpenAI ì œê³µì—…ì²´
class OpenAIProvider extends BaseAIProvider {
  async generateCompletion(options: AIRequestOptions): Promise<AIResponse> {
    const startTime = Date.now()

    try {
      // Rate Limiting ì²´í¬
      if (options.user_id) {
        await this.checkRateLimit(options.user_id)
      }

      // API í‚¤ê°€ ìˆìœ¼ë©´ ì‹¤ì œ API í˜¸ì¶œ, ì—†ìœ¼ë©´ Mock
      if (this.config.api_key && this.config.api_key !== 'sk-your-openai-key-here') {
        return await this.callOpenAIAPI(options, startTime)
      } else {
        return await this.generateMockResponse(options, startTime)
      }

    } catch (error) {
      throw new AIProviderError(
        `OpenAI API Error: ${error}`,
        'openai',
        this.config.model_id,
        500,
        true
      )
    }
  }

  private async callOpenAIAPI(options: AIRequestOptions, startTime: number): Promise<AIResponse> {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.config.api_key}`
      },
      body: JSON.stringify({
        model: this.config.model_id,
        messages: options.messages,
        max_tokens: options.max_tokens || this.config.max_tokens,
        temperature: options.temperature || 0.7,
        top_p: options.top_p || 1.0
      })
    })

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}))
      throw new Error(`HTTP ${response.status}: ${errorData.error?.message || response.statusText}`)
    }

    const data = await response.json()
    const usage = data.usage || {}
    const cost = this.calculateCost(usage.prompt_tokens || 0, usage.completion_tokens || 0)

    const aiResponse: AIResponse = {
      content: data.choices?.[0]?.message?.content || 'No response content',
      model: this.config.model_id,
      usage: {
        input_tokens: usage.prompt_tokens || 0,
        output_tokens: usage.completion_tokens || 0,
        total_tokens: usage.total_tokens || 0
      },
      cost,
      response_time: Date.now() - startTime,
      finish_reason: data.choices?.[0]?.finish_reason || 'stop'
    }

    // ì‚¬ìš©ëŸ‰ ê¸°ë¡
    if (options.user_id) {
      await ApiUsageService.recordUsageBatch([{
        userId: options.user_id!,
        model: this.config.model_id,
        inputTokens: aiResponse.usage.input_tokens,
        outputTokens: aiResponse.usage.output_tokens,
        cost: aiResponse.cost
      }])
    }

    return aiResponse
  }

  private async generateMockResponse(options: AIRequestOptions, startTime: number): Promise<AIResponse> {
    // Mock API ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜
    await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 2000))

    const inputText = options.messages.map(m => m.content).join(' ')
    const inputTokens = this.estimateTokens(inputText)
    const outputTokens = Math.floor(Math.random() * 500) + 100
    const mockContent = `[MOCK] OpenAI ${this.config.model_id} ì‘ë‹µ ì˜ˆì‹œ:

ìš”ì²­í•˜ì‹  ë¶„ì„ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ë‚´ìš©ë“¤ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **í•µì‹¬ ë¶„ì„ ê²°ê³¼**: ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.
2. **ê¶Œì¥ì‚¬í•­**: ë°ì´í„° ê¸°ë°˜ì˜ ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­ì„ ì œì‹œí•©ë‹ˆë‹¤.
3. **ë‹¤ìŒ ë‹¨ê³„**: êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íšê³¼ ìš°ì„ ìˆœìœ„ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

â€» ì´ëŠ” Mock ì‘ë‹µì…ë‹ˆë‹¤. ì‹¤ì œ OpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ë©´ ì •í™•í•œ AI ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.`

    const response: AIResponse = {
      content: mockContent,
      model: this.config.model_id,
      usage: {
        input_tokens: inputTokens,
        output_tokens: outputTokens,
        total_tokens: inputTokens + outputTokens
      },
      cost: this.calculateCost(inputTokens, outputTokens),
      response_time: Date.now() - startTime,
      finish_reason: 'stop'
    }

    // ì‚¬ìš©ëŸ‰ ê¸°ë¡
    if (options.user_id) {
      await ApiUsageService.recordUsageBatch([{
        userId: options.user_id!,
        model: this.config.model_id,
        inputTokens: inputTokens,
        outputTokens: outputTokens,
        cost: response.cost
      }])
    }

    return response
  }
}

// Anthropic ì œê³µì—…ì²´
class AnthropicProvider extends BaseAIProvider {
  async generateCompletion(options: AIRequestOptions): Promise<AIResponse> {
    const startTime = Date.now()

    try {
      // Rate Limiting ì²´í¬
      if (options.user_id) {
        await this.checkRateLimit(options.user_id)
      }

      // API ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜
      await new Promise(resolve => setTimeout(resolve, 1500 + Math.random() * 2500))

      const inputText = options.messages.map(m => m.content).join(' ')
      const inputTokens = this.estimateTokens(inputText)
      const outputTokens = Math.floor(Math.random() * 400) + 80
      const mockContent = `Anthropic ${this.config.model_id} response to: ${inputText.substring(0, 50)}...`

      const response: AIResponse = {
        content: mockContent,
        model: this.config.model_id,
        usage: {
          input_tokens: inputTokens,
          output_tokens: outputTokens,
          total_tokens: inputTokens + outputTokens
        },
        cost: this.calculateCost(inputTokens, outputTokens),
        response_time: Date.now() - startTime,
        finish_reason: 'stop'
      }

      // ì‚¬ìš©ëŸ‰ ê¸°ë¡
      if (options.user_id) {
        const cost = this.calculateCost(inputTokens, outputTokens)
        await ApiUsageService.recordUsageBatch([{
          userId: options.user_id!,
          model: this.config.model_id,
          inputTokens: inputTokens,
          outputTokens: outputTokens,
          cost
        }])
      }

      return response

    } catch (error) {
      throw new AIProviderError(
        `Anthropic API Error: ${error}`,
        'anthropic',
        this.config.model_id,
        500,
        true
      )
    }
  }
}

// Google ì œê³µì—…ì²´
class GoogleProvider extends BaseAIProvider {
  async generateCompletion(options: AIRequestOptions): Promise<AIResponse> {
    const startTime = Date.now()

    try {
      // Rate Limiting ì²´í¬
      if (options.user_id) {
        await this.checkRateLimit(options.user_id)
      }

      // API ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜
      await new Promise(resolve => setTimeout(resolve, 2000 + Math.random() * 3000))

      const inputText = options.messages.map(m => m.content).join(' ')
      const inputTokens = this.estimateTokens(inputText)
      const outputTokens = Math.floor(Math.random() * 300) + 60
      const mockContent = `Google ${this.config.model_id} response to: ${inputText.substring(0, 50)}...`

      const response: AIResponse = {
        content: mockContent,
        model: this.config.model_id,
        usage: {
          input_tokens: inputTokens,
          output_tokens: outputTokens,
          total_tokens: inputTokens + outputTokens
        },
        cost: this.calculateCost(inputTokens, outputTokens),
        response_time: Date.now() - startTime,
        finish_reason: 'stop'
      }

      // ì‚¬ìš©ëŸ‰ ê¸°ë¡
      if (options.user_id) {
        const cost = this.calculateCost(inputTokens, outputTokens)
        await ApiUsageService.recordUsageBatch([{
          userId: options.user_id!,
          model: this.config.model_id,
          inputTokens: inputTokens,
          outputTokens: outputTokens,
          cost
        }])
      }

      return response

    } catch (error) {
      throw new AIProviderError(
        `Google API Error: ${error}`,
        'google',
        this.config.model_id,
        500,
        true
      )
    }
  }
}

// ì»¤ìŠ¤í…€ ì œê³µì—…ì²´
class CustomProvider extends BaseAIProvider {
  async generateCompletion(options: AIRequestOptions): Promise<AIResponse> {
    const startTime = Date.now()

    try {
      // Rate Limiting ì²´í¬
      if (options.user_id) {
        await this.checkRateLimit(options.user_id)
      }

      // ì»¤ìŠ¤í…€ API ìš”ì²­
      const response = await fetch(this.config.api_endpoint || '', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.config.api_key}`
        },
        body: JSON.stringify({
          model: this.config.model_id,
          messages: options.messages,
          max_tokens: options.max_tokens || this.config.max_tokens,
          temperature: options.temperature || 0.7
        })
      })

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`)
      }

      const data = await response.json()
      const inputText = options.messages.map(m => m.content).join(' ')
      const inputTokens = this.estimateTokens(inputText)
      const outputTokens = this.estimateTokens(data.content || '')

      const aiResponse: AIResponse = {
        content: data.content || data.message || 'Custom API response',
        model: this.config.model_id,
        usage: {
          input_tokens: inputTokens,
          output_tokens: outputTokens,
          total_tokens: inputTokens + outputTokens
        },
        cost: this.calculateCost(inputTokens, outputTokens),
        response_time: Date.now() - startTime,
        finish_reason: 'stop'
      }

      // ì‚¬ìš©ëŸ‰ ê¸°ë¡
      if (options.user_id) {
        const cost = this.calculateCost(inputTokens, outputTokens)
        await ApiUsageService.recordUsageBatch([{
          userId: options.user_id!,
          model: this.config.model_id,
          inputTokens: inputTokens,
          outputTokens: outputTokens,
          cost
        }])
      }

      return aiResponse

    } catch (error) {
      throw new AIProviderError(
        `Custom API Error: ${error}`,
        'custom',
        this.config.model_id,
        500,
        true
      )
    }
  }
}

// AI ì œê³µì—…ì²´ íŒ©í† ë¦¬ í´ë˜ìŠ¤
export class AIProviderFactory {
  private static models: Map<string, AIModelConfig> = new Map()
  private static providers: Map<string, BaseAIProvider> = new Map()
  private static fallbackConfig: FallbackConfig = {
    enabled: true,
    models: [],
    max_retries: 3,
    retry_delay: 1000
  }

  // ëª¨ë¸ ë“±ë¡
  static registerModel(config: AIModelConfig): void {
    this.models.set(config.id, config)

    // ì œê³µì—…ì²´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    let provider: BaseAIProvider
    switch (config.provider) {
      case 'openai':
        provider = new OpenAIProvider(config)
        break
      case 'anthropic':
        provider = new AnthropicProvider(config)
        break
      case 'google':
        provider = new GoogleProvider(config)
        break
      case 'custom':
        provider = new CustomProvider(config)
        break
      default:
        throw new Error(`Unsupported provider: ${config.provider}`)
    }

    this.providers.set(config.id, provider)
  }

  // ëª¨ë¸ ì œê±°
  static unregisterModel(modelId: string): void {
    this.models.delete(modelId)
    this.providers.delete(modelId)
  }

  // ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
  static getRegisteredModels(): AIModelConfig[] {
    return Array.from(this.models.values())
  }

  // íŠ¹ì • ëª¨ë¸ ì¡°íšŒ
  static getModel(modelId: string): AIModelConfig | undefined {
    return this.models.get(modelId)
  }

  // í´ë°± ì„¤ì •
  static setFallbackConfig(config: Partial<FallbackConfig>): void {
    this.fallbackConfig = { ...this.fallbackConfig, ...config }
  }

  // AI ì™„ë£Œ ìš”ì²­ (í´ë°± ì§€ì›)
  static async generateCompletion(
    modelId: string,
    options: AIRequestOptions
  ): Promise<AIResponse> {
    const modelsToTry = this.fallbackConfig.enabled
      ? [modelId, ...this.fallbackConfig.models.filter(id => id !== modelId)]
      : [modelId]

    let lastError: Error | null = null

    for (let i = 0; i < modelsToTry.length && i < this.fallbackConfig.max_retries; i++) {
      const currentModelId = modelsToTry[i]
      const provider = this.providers.get(currentModelId)

      if (!provider) {
        lastError = new Error(`Model not found: ${currentModelId}`)
        continue
      }

      try {
        console.log(`Attempting to use model: ${currentModelId}`)
        const response = await provider.generateCompletion(options)

        if (i > 0) {
          console.log(`Fallback successful with model: ${currentModelId}`)
        }

        return response

      } catch (error) {
        lastError = error as Error
        console.warn(`Model ${currentModelId} failed:`, error)

        // ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬ê°€ ì•„ë‹ˆë©´ ì¦‰ì‹œ ì¤‘ë‹¨
        if (error instanceof AIProviderError && !error.retryable) {
          break
        }

        // ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì ì‹œ ëŒ€ê¸°
        if (i < modelsToTry.length - 1) {
          await new Promise(resolve =>
            setTimeout(resolve, this.fallbackConfig.retry_delay * (i + 1))
          )
        }
      }
    }

    // ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
    throw lastError || new Error('All AI providers failed')
  }

  // í—¬ìŠ¤ ì²´í¬
  static async healthCheck(modelId?: string): Promise<Record<string, boolean>> {
    const modelsToCheck = modelId ? [modelId] : Array.from(this.models.keys())
    const results: Record<string, boolean> = {}

    await Promise.allSettled(
      modelsToCheck.map(async (id) => {
        try {
          const provider = this.providers.get(id)
          if (!provider) {
            results[id] = false
            return
          }

          // ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
          await provider.generateCompletion({
            messages: [{ role: 'user', content: 'test' }],
            max_tokens: 10
          })
          results[id] = true
        } catch {
          results[id] = false
        }
      })
    )

    return results
  }

  // ì œê³µì—…ì²´ë³„ í†µê³„
  static getProviderStats(): Record<AIProvider, { models: number; active: number }> {
    const stats: Record<AIProvider, { models: number; active: number }> = {
      openai: { models: 0, active: 0 },
      anthropic: { models: 0, active: 0 },
      google: { models: 0, active: 0 },
      custom: { models: 0, active: 0 }
    }

    this.models.forEach(model => {
      stats[model.provider].models++
      if (this.providers.has(model.id)) {
        stats[model.provider].active++
      }
    })

    return stats
  }

  // ëª¨ë“  ì œê³µì—…ì²´ ì´ˆê¸°í™”
  static clear(): void {
    this.models.clear()
    this.providers.clear()
  }
}

// ê¸°ë³¸ ëª¨ë¸ë“¤ ë“±ë¡
export function initializeDefaultModels(): void {
  // í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸°
  const openaiApiKey = import.meta.env.VITE_OPENAI_API_KEY
  const anthropicApiKey = import.meta.env.VITE_ANTHROPIC_API_KEY
  const googleApiKey = import.meta.env.VITE_GOOGLE_AI_API_KEY

  console.log('ğŸ”‘ AI API í‚¤ í™•ì¸:')
  console.log('OpenAI:', openaiApiKey ? 'âœ… ì„¤ì •ë¨' : 'âŒ ëˆ„ë½')
  console.log('Anthropic:', anthropicApiKey ? 'âœ… ì„¤ì •ë¨' : 'âŒ ëˆ„ë½')
  console.log('Google:', googleApiKey ? 'âœ… ì„¤ì •ë¨' : 'âŒ ëˆ„ë½')

  const defaultModels: AIModelConfig[] = []

  // OpenAI ëª¨ë¸ë“¤ (API í‚¤ê°€ ìˆì„ ë•Œë§Œ ë“±ë¡)
  if (openaiApiKey && openaiApiKey !== 'sk-your-openai-key-here') {
    defaultModels.push(
      {
        id: 'gpt-4o',
        name: 'GPT-4o',
        provider: 'openai',
        model_id: 'gpt-4o',
        api_key: openaiApiKey,
        max_tokens: 128000,
        cost_per_input_token: 0.000005,
        cost_per_output_token: 0.000015,
        rate_limits: { requests_per_minute: 500, tokens_per_minute: 30000 }
      },
      {
        id: 'gpt-4-turbo',
        name: 'GPT-4 Turbo',
        provider: 'openai',
        model_id: 'gpt-4-turbo-preview',
        api_key: openaiApiKey,
        max_tokens: 4096,
        cost_per_input_token: 0.00001,
        cost_per_output_token: 0.00003,
        rate_limits: { requests_per_minute: 500, tokens_per_minute: 30000 }
      }
    )
  }

  // Anthropic ëª¨ë¸ë“¤ (API í‚¤ê°€ ìˆì„ ë•Œë§Œ ë“±ë¡)
  if (anthropicApiKey && anthropicApiKey !== 'your-anthropic-key-here') {
    defaultModels.push(
      {
        id: 'claude-3-opus',
        name: 'Claude 3 Opus',
        provider: 'anthropic',
        model_id: 'claude-3-opus-20240229',
        api_key: anthropicApiKey,
        max_tokens: 4096,
        cost_per_input_token: 0.000015,
        cost_per_output_token: 0.000075,
        rate_limits: { requests_per_minute: 100, tokens_per_minute: 10000 }
      },
      {
        id: 'claude-3-sonnet',
        name: 'Claude 3 Sonnet',
        provider: 'anthropic',
        model_id: 'claude-3-sonnet-20240229',
        api_key: anthropicApiKey,
        max_tokens: 4096,
        cost_per_input_token: 0.000003,
        cost_per_output_token: 0.000015,
        rate_limits: { requests_per_minute: 300, tokens_per_minute: 20000 }
      }
    )
  }

  // Google ëª¨ë¸ë“¤ (API í‚¤ê°€ ìˆì„ ë•Œë§Œ ë“±ë¡)
  if (googleApiKey && googleApiKey !== 'your-google-ai-key-here') {
    defaultModels.push({
      id: 'gemini-pro',
      name: 'Gemini Pro',
      provider: 'google',
      model_id: 'gemini-pro',
      api_key: googleApiKey,
      max_tokens: 2048,
      cost_per_input_token: 0.0000005,
      cost_per_output_token: 0.0000015,
      rate_limits: { requests_per_minute: 60, tokens_per_minute: 5000 }
    })
  }

  // ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê²½ê³  ë©”ì‹œì§€
  if (defaultModels.length === 0) {
    console.warn('âš ï¸ AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Mock ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.')
    // Mock ëª¨ë¸ ì¶”ê°€ (ê°œë°œìš©)
    defaultModels.push({
      id: 'mock-gpt-4o',
      name: 'GPT-4o (Mock)',
      provider: 'openai',
      model_id: 'gpt-4o',
      max_tokens: 128000,
      cost_per_input_token: 0.000005,
      cost_per_output_token: 0.000015,
      rate_limits: { requests_per_minute: 500, tokens_per_minute: 30000 }
    })
  }

  defaultModels.forEach(model => {
    AIProviderFactory.registerModel(model)
  })

  // í´ë°± ì²´ì¸ ì„¤ì • (ë“±ë¡ëœ ëª¨ë¸ë“¤ë¡œë§Œ êµ¬ì„±)
  const availableModelIds = defaultModels.map(model => model.id)
  AIProviderFactory.setFallbackConfig({
    enabled: true,
    models: availableModelIds,
    max_retries: 3,
    retry_delay: 1000
  })

  console.log(`âœ… ${defaultModels.length}ê°œì˜ AI ëª¨ë¸ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤:`, availableModelIds)
}

// íŒ©í† ë¦¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê¸°ë³¸ ë‚´ë³´ë‚´ê¸°
export const aiProviderFactory = AIProviderFactory