import { ApiUsageService } from '../apiUsageService'
import { RateLimiter } from '../../lib/rateLimiter'

// ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ë“¤ì„ staticìœ¼ë¡œ ì‚¬ìš©

// AI ì œê³µì—…ì²´ íƒ€ì… ì •ì˜
export type AIProvider = 'openai' | 'anthropic' | 'google' | 'custom'

// AI ìš”ì²­ ë©”ì‹œì§€ íƒ€ì…
export interface AIMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

// AI ëª¨ë¸ ì„¤ì • ì¸í„°í˜ì´ìŠ¤
export interface AIModelConfig {
  id: string
  name: string
  provider: AIProvider
  model_id: string
  api_key?: string
  api_endpoint?: string
  max_tokens: number
  temperature?: number
  top_p?: number
  cost_per_input_token: number
  cost_per_output_token: number
  rate_limits: {
    requests_per_minute: number
    tokens_per_minute: number
  }
}

// AI ì‘ë‹µ ì¸í„°í˜ì´ìŠ¤
export interface AIResponse {
  content: string
  model: string
  usage: {
    input_tokens: number
    output_tokens: number
    total_tokens: number
  }
  cost: number
  response_time: number
  finish_reason: 'stop' | 'length' | 'error'
}

// AI ìš”ì²­ ì˜µì…˜
export interface AIRequestOptions {
  messages: AIMessage[]
  max_tokens?: number
  temperature?: number
  top_p?: number
  stream?: boolean
  user_id?: string
}

// í´ë°± ì„¤ì •
export interface FallbackConfig {
  enabled: boolean
  models: string[] // ìš°ì„ ìˆœìœ„ëŒ€ë¡œ ì •ë ¬ëœ ëª¨ë¸ ID ëª©ë¡
  max_retries: number
  retry_delay: number // ms
}

// ì—ëŸ¬ íƒ€ì…
export class AIProviderError extends Error {
  constructor(
    message: string,
    public provider: AIProvider,
    public model: string,
    public statusCode?: number,
    public retryable: boolean = false
  ) {
    super(message)
    this.name = 'AIProviderError'
  }
}

// ê¸°ë³¸ AI ì œê³µì—…ì²´ í´ë˜ìŠ¤
abstract class BaseAIProvider {
  protected config: AIModelConfig

  constructor(config: AIModelConfig) {
    this.config = config
  }

  abstract generateCompletion(options: AIRequestOptions): Promise<AIResponse>

  // ê³µí†µ í† í° ê³„ì‚° ë©”ì„œë“œ
  protected estimateTokens(text: string): number {
    // ê°„ë‹¨í•œ í† í° ì¶”ì • - ì‹¤ì œë¡œëŠ” ëª¨ë¸ë³„ í† í¬ë‚˜ì´ì € ì‚¬ìš©
    return Math.ceil(text.length / 4)
  }

  // ë¹„ìš© ê³„ì‚°
  protected calculateCost(inputTokens: number, outputTokens: number): number {
    return (inputTokens * this.config.cost_per_input_token) +
           (outputTokens * this.config.cost_per_output_token)
  }

  // Rate Limiting ì²´í¬
  protected async checkRateLimit(userId: string): Promise<void> {
    const allowed = await RateLimiter.checkRateLimit(userId, 'user', 1)
    if (!allowed.allowed) {
      throw new AIProviderError(
        `Rate limit exceeded: ${allowed.reason}`,
        this.config.provider,
        this.config.model_id,
        429,
        true
      )
    }
  }
}

// OpenAI ì œê³µì—…ì²´
class OpenAIProvider extends BaseAIProvider {
  async generateCompletion(options: AIRequestOptions): Promise<AIResponse> {
    const startTime = Date.now()

    try {
      // Rate Limiting ì²´í¬
      if (options.user_id) {
        await this.checkRateLimit(options.user_id)
      }

      // í™˜ê²½ë³„ API í‚¤ í™•ì¸
      const isDev = import.meta.env.DEV

      if (isDev) {
        // ê°œë°œ í™˜ê²½: í´ë¼ì´ì–¸íŠ¸ì‚¬ì´ë“œ API í‚¤ ì²´í¬
        if (!this.config.api_key || this.config.api_key === 'sk-your-openai-key-here' || !this.config.api_key.startsWith('sk-')) {
          console.error('âŒ OpenAI API í‚¤ ì˜¤ë¥˜:', {
            hasKey: !!this.config.api_key,
            keyPrefix: this.config.api_key?.substring(0, 10),
            keyLength: this.config.api_key?.length
          })
          throw new AIProviderError(
            'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ VITE_OPENAI_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.',
            'openai',
            this.config.model_id,
            401,
            false
          )
        }
      } else {
        // í”„ë¡œë•ì…˜ í™˜ê²½: ì„œë²„ì‚¬ì´ë“œ API Routes ì‚¬ìš© (í´ë¼ì´ì–¸íŠ¸ í‚¤ ì²´í¬ ê±´ë„ˆë›°ê¸°)
        console.log('ğŸ”„ í”„ë¡œë•ì…˜ í™˜ê²½ - OpenAI ì„œë²„ì‚¬ì´ë“œ API Routes ì‚¬ìš©:', '/api/openai')
      }

      return await this.callOpenAIAPI(options, startTime)

    } catch (error) {
      if (error instanceof AIProviderError) {
        throw error
      }
      throw new AIProviderError(
        `OpenAI API Error: ${error}`,
        'openai',
        this.config.model_id,
        500,
        true
      )
    }
  }

  private async callOpenAIAPI(options: AIRequestOptions, startTime: number): Promise<AIResponse> {
    // ê°„ë‹¨í•œ API Routes ê²½ë¡œ ì‚¬ìš©
    const apiUrl = '/api/openai'

    console.log('ğŸŒ OpenAI API URL:', apiUrl, '(dev mode:', import.meta.env.DEV, ')')

    // í™˜ê²½ë³„ í—¤ë” ì„¤ì •
    const isDev = import.meta.env.DEV
    const headers: Record<string, string> = {
      'Content-Type': 'application/json'
    }

    // ê°œë°œ í™˜ê²½ì—ì„œë§Œ í´ë¼ì´ì–¸íŠ¸ì‚¬ì´ë“œ API í‚¤ ì‚¬ìš©
    if (isDev && this.config.api_key) {
      headers['Authorization'] = `Bearer ${this.config.api_key}`
      console.log('ğŸ”‘ ê°œë°œ í™˜ê²½ - OpenAI í´ë¼ì´ì–¸íŠ¸ì‚¬ì´ë“œ API í‚¤ ì‚¬ìš©')
    } else {
      console.log('ğŸ”„ í”„ë¡œë•ì…˜ í™˜ê²½ - OpenAI ì„œë²„ì‚¬ì´ë“œ API í‚¤ ì‚¬ìš©')
    }

    const response = await fetch(apiUrl, {
      method: 'POST',
      headers,
      body: JSON.stringify({
        model: this.config.model_id,
        messages: options.messages,
        max_tokens: options.max_tokens || this.config.max_tokens,
        temperature: options.temperature || 0.7,
        top_p: options.top_p || 1.0
      })
    })

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}))
      throw new Error(`HTTP ${response.status}: ${errorData.error?.message || response.statusText}`)
    }

    const data = await response.json()
    const usage = data.usage || {}
    const cost = this.calculateCost(usage.prompt_tokens || 0, usage.completion_tokens || 0)

    const aiResponse: AIResponse = {
      content: data.choices?.[0]?.message?.content || 'No response content',
      model: this.config.model_id,
      usage: {
        input_tokens: usage.prompt_tokens || 0,
        output_tokens: usage.completion_tokens || 0,
        total_tokens: usage.total_tokens || 0
      },
      cost,
      response_time: Date.now() - startTime,
      finish_reason: data.choices?.[0]?.finish_reason || 'stop'
    }

    // ì‚¬ìš©ëŸ‰ ê¸°ë¡
    if (options.user_id) {
      await ApiUsageService.recordUsageBatch([{
        userId: options.user_id!,
        model: this.config.model_id,
        inputTokens: aiResponse.usage.input_tokens,
        outputTokens: aiResponse.usage.output_tokens,
        cost: aiResponse.cost
      }])
    }

    return aiResponse
  }

}

// Anthropic ì œê³µì—…ì²´
class AnthropicProvider extends BaseAIProvider {
  async generateCompletion(options: AIRequestOptions): Promise<AIResponse> {
    const startTime = Date.now()

    try {
      // Rate Limiting ì²´í¬
      if (options.user_id) {
        await this.checkRateLimit(options.user_id)
      }

      // í™˜ê²½ë³„ API í‚¤ í™•ì¸
      const isDev = import.meta.env.DEV

      if (isDev) {
        // ê°œë°œ í™˜ê²½: í´ë¼ì´ì–¸íŠ¸ì‚¬ì´ë“œ API í‚¤ ì²´í¬
        if (!this.config.api_key || this.config.api_key === 'your-anthropic-key-here') {
          throw new AIProviderError(
            'Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ VITE_ANTHROPIC_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.',
            'anthropic',
            this.config.model_id,
            401,
            false
          )
        }
      } else {
        // í”„ë¡œë•ì…˜ í™˜ê²½: ì„œë²„ì‚¬ì´ë“œ API Routes ì‚¬ìš© (í´ë¼ì´ì–¸íŠ¸ í‚¤ ì²´í¬ ê±´ë„ˆë›°ê¸°)
        console.log('ğŸ”„ í”„ë¡œë•ì…˜ í™˜ê²½ - ì„œë²„ì‚¬ì´ë“œ API Routes ì‚¬ìš©:', '/api/anthropic')
      }

      return await this.callAnthropicAPI(options, startTime)

    } catch (error) {
      if (error instanceof AIProviderError) {
        throw error
      }
      throw new AIProviderError(
        `Anthropic API Error: ${error}`,
        'anthropic',
        this.config.model_id,
        500,
        true
      )
    }
  }

  private async callAnthropicAPI(options: AIRequestOptions, startTime: number): Promise<AIResponse> {
    try {
      // Anthropic API ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜ - ë” ì—„ê²©í•œ ìœ íš¨ì„± ê²€ì‚¬
      const anthropicMessages = options.messages
        .filter(m => m.role !== 'system' && m.content && m.content.trim())
        .map(m => ({
          role: m.role === 'user' ? 'user' as const : 'assistant' as const,
          content: m.content.trim()
        }))

      // ìµœì†Œ 1ê°œì˜ ë©”ì‹œì§€ê°€ í•„ìš”í•¨
      if (anthropicMessages.length === 0) {
        throw new Error('ìµœì†Œ 1ê°œì˜ ìœ íš¨í•œ ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.')
      }

      const systemMessage = options.messages.find(m => m.role === 'system')?.content?.trim() || ''

      const requestBody = {
        model: this.config.model_id,
        max_tokens: Math.min(options.max_tokens || this.config.max_tokens, 4096),
        temperature: Math.max(0, Math.min(1, options.temperature || 0.7)),
        messages: anthropicMessages,
        ...(systemMessage && { system: systemMessage })
      }

      console.log('ğŸ” Anthropic API ìš”ì²­ ì„¸ë¶€ì •ë³´:', {
        model: this.config.model_id,
        messageCount: anthropicMessages.length,
        hasSystem: !!systemMessage,
        apiKeyPrefix: this.config.api_key?.substring(0, 10) + '...',
        apiKeyLength: this.config.api_key?.length,
        requestBodySize: JSON.stringify(requestBody).length
      })

      // Anthropic API í‚¤ ìœ íš¨ì„± ì¬í™•ì¸
      // í™˜ê²½ë³„ API í‚¤ ìµœì¢… ê²€ì¦
      const isDev = import.meta.env.DEV
      if (isDev) {
        // ê°œë°œ í™˜ê²½ì—ì„œë§Œ API í‚¤ ê²€ì¦
        if (!this.config.api_key || this.config.api_key === 'your-anthropic-key-here' || !this.config.api_key.startsWith('sk-ant-')) {
          throw new Error(`ì˜ëª»ëœ Anthropic API í‚¤ì…ë‹ˆë‹¤. í‚¤ í˜•ì‹: ${this.config.api_key?.substring(0, 10)}...`)
        }
      }

      // í™˜ê²½ì— ë”°ë¥¸ API ì—”ë“œí¬ì¸íŠ¸ ê²°ì • - í”„ë¡œë•ì…˜ ëŒ€ì‘ ê°•í™”

      let apiUrl: string
      if (isDev) {
        // ê°œë°œ í™˜ê²½: Vite í”„ë¡ì‹œ ì‚¬ìš©
        apiUrl = '/proxy/anthropic'
      } else {
        // í”„ë¡œë•ì…˜ í™˜ê²½: Vercel API Routes ì‚¬ìš©
        apiUrl = '/api/anthropic'
      }

      console.log('ğŸŒ Anthropic API í˜¸ì¶œ ì„¤ì •:', {
        environment: isDev ? 'development' : 'production',
        apiUrl,
        modelId: this.config.model_id,
        requestSize: JSON.stringify(requestBody).length
      })

      // í™˜ê²½ë³„ í—¤ë” ì„¤ì •
      const headers: Record<string, string> = {
        'Content-Type': 'application/json',
        'anthropic-version': '2024-10-22',
        'User-Agent': 'ELUO-Project/1.0'
      }

      // ê°œë°œ í™˜ê²½ì—ì„œë§Œ í´ë¼ì´ì–¸íŠ¸ì‚¬ì´ë“œ API í‚¤ ì‚¬ìš©
      if (isDev && this.config.api_key) {
        headers['x-api-key'] = this.config.api_key
        console.log('ğŸ”‘ ê°œë°œ í™˜ê²½ - í´ë¼ì´ì–¸íŠ¸ì‚¬ì´ë“œ API í‚¤ ì‚¬ìš©')
      } else {
        console.log('ğŸ”„ í”„ë¡œë•ì…˜ í™˜ê²½ - ì„œë²„ì‚¬ì´ë“œ API í‚¤ ì‚¬ìš©')
      }

      // íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ - ë¬¸ì„œ ë¶„ì„ì— ë” ë§ì€ ì‹œê°„ í•„ìš”
      const controller = new AbortController()
      const timeoutId = setTimeout(() => {
        console.warn('âš ï¸ Anthropic API íƒ€ì„ì•„ì›ƒ ë°œìƒ')
        controller.abort()
      }, 120000) // 120ì´ˆ íƒ€ì„ì•„ì›ƒ (ë¬¸ì„œ ë¶„ì„ì€ ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)

      try {
        const response = await fetch(apiUrl, {
          method: 'POST',
          headers,
          body: JSON.stringify(requestBody),
          signal: controller.signal
        })

        clearTimeout(timeoutId)

        console.log('ğŸ“¡ Anthropic API ì‘ë‹µ ìˆ˜ì‹ :', {
          status: response.status,
          statusText: response.statusText,
          ok: response.ok,
          contentType: response.headers.get('content-type')
        })

        if (!response.ok) {
          const errorText = await response.text()
          console.error('âŒ Anthropic API ì—ëŸ¬ ì‘ë‹µ:', {
            status: response.status,
            statusText: response.statusText,
            errorBody: errorText,
            headers: Object.fromEntries(response.headers.entries())
          })

          let errorData
          try {
            errorData = JSON.parse(errorText)
          } catch (parseError) {
            console.error('âŒ ì—ëŸ¬ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:', parseError)
            errorData = { error: { message: errorText || 'ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬' } }
          }

          // ìƒì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±
          const errorMessage = errorData.error?.message ||
                             errorData.message ||
                             `Anthropic API ì˜¤ë¥˜ (HTTP ${response.status}): ${response.statusText}`

          const isRetryable = [429, 500, 502, 503, 504].includes(response.status)

          throw new AIProviderError(
            errorMessage,
            'anthropic',
            this.config.model_id,
            response.status,
            isRetryable
          )
        }

        const data = await response.json()
        console.log('âœ… Anthropic API ì„±ê³µ ì‘ë‹µ:', {
          contentLength: data.content?.[0]?.text?.length || 0,
          usage: data.usage,
          stopReason: data.stop_reason,
          modelUsed: this.config.model_id,
          responseTime: Date.now() - startTime
        })

        // Anthropic ì‘ë‹µì„ AIResponse í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        const aiResponse: AIResponse = {
          content: data.content?.[0]?.text || 'No response content',
          model: this.config.model_id,
          usage: {
            input_tokens: data.usage?.input_tokens || 0,
            output_tokens: data.usage?.output_tokens || 0,
            total_tokens: (data.usage?.input_tokens || 0) + (data.usage?.output_tokens || 0)
          },
          cost: this.calculateCost(data.usage?.input_tokens || 0, data.usage?.output_tokens || 0),
          response_time: Date.now() - startTime,
          finish_reason: data.stop_reason === 'end_turn' ? 'stop' : 'length'
        }

        // ì‚¬ìš©ëŸ‰ ê¸°ë¡
        if (options.user_id) {
          await ApiUsageService.recordUsageBatch([{
            userId: options.user_id,
            model: this.config.model_id,
            inputTokens: aiResponse.usage.input_tokens,
            outputTokens: aiResponse.usage.output_tokens,
            cost: aiResponse.cost
          }])
        }

        return aiResponse
      } catch (fetchError) {
        clearTimeout(timeoutId)

        if (fetchError instanceof Error && fetchError.name === 'AbortError') {
          console.error('âŒ Anthropic API íƒ€ì„ì•„ì›ƒ')
          throw new AIProviderError(
            'Anthropic API ìš”ì²­ì´ 120ì´ˆ í›„ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì„œê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ë³µì¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
            'anthropic',
            this.config.model_id,
            408,
            true
          )
        }

        console.error('ğŸš¨ Anthropic API Fetch ì˜¤ë¥˜:', {
          name: fetchError instanceof Error ? fetchError.name : 'Unknown',
          message: fetchError instanceof Error ? fetchError.message : String(fetchError),
          stack: fetchError instanceof Error ? fetchError.stack : undefined
        })

        throw new AIProviderError(
          `Anthropic API ì—°ê²° ì˜¤ë¥˜: ${fetchError instanceof Error ? fetchError.message : String(fetchError)}`,
          'anthropic',
          this.config.model_id,
          0,
          true
        )
      }
    } catch (error) {
      console.error('âŒ Anthropic ì „ì²´ ì˜¤ë¥˜:', error)

      if (error instanceof AIProviderError) {
        throw error
      }

      throw new AIProviderError(
        `Anthropic API ì²˜ë¦¬ ì˜¤ë¥˜: ${error instanceof Error ? error.message : String(error)}`,
        'anthropic',
        this.config.model_id,
        0,
        false
      )
    }
  }
}

// Google ì œê³µì—…ì²´
class GoogleProvider extends BaseAIProvider {
  async generateCompletion(options: AIRequestOptions): Promise<AIResponse> {
    const startTime = Date.now()

    try {
      // Rate Limiting ì²´í¬
      if (options.user_id) {
        await this.checkRateLimit(options.user_id)
      }

      // API í‚¤ í™•ì¸ - ë°˜ë“œì‹œ ì‹¤ì œ API í‚¤ê°€ ìˆì–´ì•¼ í•¨
      if (!this.config.api_key || this.config.api_key === 'your-google-ai-key-here') {
        console.error('âŒ Google AI API í‚¤ ì˜¤ë¥˜:', {
          hasKey: !!this.config.api_key,
          keyPrefix: this.config.api_key?.substring(0, 10),
          keyLength: this.config.api_key?.length
        })
        throw new AIProviderError(
          'Google AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ VITE_GOOGLE_AI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.',
          'google',
          this.config.model_id,
          401,
          false
        )
      }

      return await this.callGoogleAIAPI(options, startTime)

    } catch (error) {
      if (error instanceof AIProviderError) {
        throw error
      }
      throw new AIProviderError(
        `Google AI API Error: ${error}`,
        'google',
        this.config.model_id,
        500,
        true
      )
    }
  }

  private async callGoogleAIAPI(options: AIRequestOptions, startTime: number): Promise<AIResponse> {
    // Google AI API ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜
    const parts = options.messages
      .filter(m => m.role !== 'system')
      .map(m => ({ text: m.content }))

    const systemMessage = options.messages.find(m => m.role === 'system')?.content

    const requestBody = {
      contents: [{
        parts: parts
      }],
      generationConfig: {
        temperature: options.temperature || 0.7,
        topP: options.top_p || 1.0,
        maxOutputTokens: options.max_tokens || this.config.max_tokens,
      }
    }

    // ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if (systemMessage) {
      requestBody.contents.unshift({
        parts: [{ text: systemMessage }]
      })
    }

    // ê°œë°œ í™˜ê²½ê³¼ Vercel í™˜ê²½ ëª¨ë‘ í”„ë¡ì‹œ/API Routes ì‚¬ìš©
    const apiUrl = `/api/google/v1beta/models/${this.config.model_id}:generateContent?key=${this.config.api_key}`

    console.log('ğŸŒ Google AI API URL:', apiUrl, '(dev mode:', import.meta.env.DEV, ')')

    const response = await fetch(apiUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(requestBody)
    })

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}))
      throw new Error(`HTTP ${response.status}: ${errorData.error?.message || response.statusText}`)
    }

    const data = await response.json()

    // Google AI API ì‘ë‹µ êµ¬ì¡° ì²˜ë¦¬
    const content = data.candidates?.[0]?.content?.parts?.[0]?.text || 'No response content'
    const inputTokens = data.usageMetadata?.promptTokenCount || 0
    const outputTokens = data.usageMetadata?.candidatesTokenCount || 0
    const cost = this.calculateCost(inputTokens, outputTokens)

    const aiResponse: AIResponse = {
      content,
      model: this.config.model_id,
      usage: {
        input_tokens: inputTokens,
        output_tokens: outputTokens,
        total_tokens: inputTokens + outputTokens
      },
      cost,
      response_time: Date.now() - startTime,
      finish_reason: data.candidates?.[0]?.finishReason === 'STOP' ? 'stop' : 'length'
    }

    // ì‚¬ìš©ëŸ‰ ê¸°ë¡
    if (options.user_id) {
      await ApiUsageService.recordUsageBatch([{
        userId: options.user_id!,
        model: this.config.model_id,
        inputTokens: aiResponse.usage.input_tokens,
        outputTokens: aiResponse.usage.output_tokens,
        cost: aiResponse.cost
      }])
    }

    return aiResponse
  }
}

// ì»¤ìŠ¤í…€ ì œê³µì—…ì²´
class CustomProvider extends BaseAIProvider {
  async generateCompletion(options: AIRequestOptions): Promise<AIResponse> {
    const startTime = Date.now()

    try {
      // Rate Limiting ì²´í¬
      if (options.user_id) {
        await this.checkRateLimit(options.user_id)
      }

      // ì»¤ìŠ¤í…€ API ìš”ì²­
      const response = await fetch(this.config.api_endpoint || '', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.config.api_key}`
        },
        body: JSON.stringify({
          model: this.config.model_id,
          messages: options.messages,
          max_tokens: options.max_tokens || this.config.max_tokens,
          temperature: options.temperature || 0.7
        })
      })

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`)
      }

      const data = await response.json()
      const inputText = options.messages.map(m => m.content).join(' ')
      const inputTokens = this.estimateTokens(inputText)
      const outputTokens = this.estimateTokens(data.content || '')

      const aiResponse: AIResponse = {
        content: data.content || data.message || 'Custom API response',
        model: this.config.model_id,
        usage: {
          input_tokens: inputTokens,
          output_tokens: outputTokens,
          total_tokens: inputTokens + outputTokens
        },
        cost: this.calculateCost(inputTokens, outputTokens),
        response_time: Date.now() - startTime,
        finish_reason: 'stop'
      }

      // ì‚¬ìš©ëŸ‰ ê¸°ë¡
      if (options.user_id) {
        const cost = this.calculateCost(inputTokens, outputTokens)
        await ApiUsageService.recordUsageBatch([{
          userId: options.user_id!,
          model: this.config.model_id,
          inputTokens: inputTokens,
          outputTokens: outputTokens,
          cost
        }])
      }

      return aiResponse

    } catch (error) {
      throw new AIProviderError(
        `Custom API Error: ${error}`,
        'custom',
        this.config.model_id,
        500,
        true
      )
    }
  }
}

// AI ì œê³µì—…ì²´ íŒ©í† ë¦¬ í´ë˜ìŠ¤
export class AIProviderFactory {
  private static models: Map<string, AIModelConfig> = new Map()
  private static providers: Map<string, BaseAIProvider> = new Map()
  private static fallbackConfig: FallbackConfig = {
    enabled: false,  // ğŸš¨ Fallback ë¹„í™œì„±í™”: ì„ íƒëœ ëª¨ë¸ë§Œ ì‚¬ìš©
    models: [],
    max_retries: 1,
    retry_delay: 1000
  }

  // ëª¨ë¸ ë“±ë¡
  static registerModel(config: AIModelConfig): void {
    this.models.set(config.id, config)

    // ì œê³µì—…ì²´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    let provider: BaseAIProvider
    switch (config.provider) {
      case 'openai':
        provider = new OpenAIProvider(config)
        break
      case 'anthropic':
        provider = new AnthropicProvider(config)
        break
      case 'google':
        provider = new GoogleProvider(config)
        break
      case 'custom':
        provider = new CustomProvider(config)
        break
      default:
        throw new Error(`Unsupported provider: ${config.provider}`)
    }

    this.providers.set(config.id, provider)
  }

  // ëª¨ë¸ ì œê±°
  static unregisterModel(modelId: string): void {
    this.models.delete(modelId)
    this.providers.delete(modelId)
  }

  // ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
  static getRegisteredModels(): AIModelConfig[] {
    return Array.from(this.models.values())
  }

  // íŠ¹ì • ëª¨ë¸ ì¡°íšŒ
  static getModel(modelId: string): AIModelConfig | undefined {
    return this.models.get(modelId)
  }

  // í´ë°± ì„¤ì •
  static setFallbackConfig(config: Partial<FallbackConfig>): void {
    this.fallbackConfig = { ...this.fallbackConfig, ...config }
  }

  // AI ì™„ë£Œ ìš”ì²­ (í´ë°± ì§€ì›)
  static async generateCompletion(
    modelId: string,
    options: AIRequestOptions
  ): Promise<AIResponse> {
    const modelsToTry = this.fallbackConfig.enabled
      ? [modelId, ...this.fallbackConfig.models.filter(id => id !== modelId)]
      : [modelId]

    let lastError: Error | null = null

    for (let i = 0; i < modelsToTry.length && i < this.fallbackConfig.max_retries; i++) {
      const currentModelId = modelsToTry[i]
      const provider = this.providers.get(currentModelId)

      if (!provider) {
        lastError = new Error(`Model not found: ${currentModelId}`)
        continue
      }

      try {
        console.log(`Attempting to use model: ${currentModelId}`)
        const response = await provider.generateCompletion(options)

        if (i > 0) {
          console.log(`Fallback successful with model: ${currentModelId}`)
        }

        return response

      } catch (error) {
        lastError = error as Error
        console.warn(`Model ${currentModelId} failed:`, error)

        // ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬ê°€ ì•„ë‹ˆë©´ ì¦‰ì‹œ ì¤‘ë‹¨
        if (error instanceof AIProviderError && !error.retryable) {
          break
        }

        // ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì ì‹œ ëŒ€ê¸°
        if (i < modelsToTry.length - 1) {
          await new Promise(resolve =>
            setTimeout(resolve, this.fallbackConfig.retry_delay * (i + 1))
          )
        }
      }
    }

    // ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
    throw lastError || new Error('All AI providers failed')
  }

  // í—¬ìŠ¤ ì²´í¬
  static async healthCheck(modelId?: string): Promise<Record<string, boolean>> {
    const modelsToCheck = modelId ? [modelId] : Array.from(this.models.keys())
    const results: Record<string, boolean> = {}

    await Promise.allSettled(
      modelsToCheck.map(async (id) => {
        try {
          const provider = this.providers.get(id)
          if (!provider) {
            results[id] = false
            return
          }

          // ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
          await provider.generateCompletion({
            messages: [{ role: 'user', content: 'test' }],
            max_tokens: 10
          })
          results[id] = true
        } catch {
          results[id] = false
        }
      })
    )

    return results
  }

  // ì œê³µì—…ì²´ë³„ í†µê³„
  static getProviderStats(): Record<AIProvider, { models: number; active: number }> {
    const stats: Record<AIProvider, { models: number; active: number }> = {
      openai: { models: 0, active: 0 },
      anthropic: { models: 0, active: 0 },
      google: { models: 0, active: 0 },
      custom: { models: 0, active: 0 }
    }

    this.models.forEach(model => {
      stats[model.provider].models++
      if (this.providers.has(model.id)) {
        stats[model.provider].active++
      }
    })

    return stats
  }

  // ëª¨ë“  ì œê³µì—…ì²´ ì´ˆê¸°í™”
  static clear(): void {
    this.models.clear()
    this.providers.clear()
  }
}

// ê¸°ë³¸ ëª¨ë¸ë“¤ ë“±ë¡
export function initializeDefaultModels(): void {
  console.log('ğŸš€ AI Provider Factory ì´ˆê¸°í™” ì‹œì‘...')

  // í™˜ê²½ë³„ API í‚¤ ì½ê¸° (ê°œë°œí™˜ê²½: VITE_, í”„ë¡œë•ì…˜: ì„œë²„ì‚¬ì´ë“œ API ì‚¬ìš©)
  const isDev = import.meta.env.DEV
  const openaiApiKey = import.meta.env.VITE_OPENAI_API_KEY
  const anthropicApiKey = import.meta.env.VITE_ANTHROPIC_API_KEY
  const googleApiKey = import.meta.env.VITE_GOOGLE_AI_API_KEY

  console.log('ğŸ”‘ AI API í‚¤ ê²€ì¦ (í™˜ê²½:', isDev ? 'ê°œë°œ' : 'í”„ë¡œë•ì…˜', '):')

  // í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” í´ë¼ì´ì–¸íŠ¸ì‚¬ì´ë“œ API í‚¤ ì²´í¬ë¥¼ ê±´ë„ˆë›°ê³  ì„œë²„ì‚¬ì´ë“œ API ì‚¬ìš©
  let openaiValid, anthropicValid, googleValid

  if (isDev) {
    // ê°œë°œ í™˜ê²½: í´ë¼ì´ì–¸íŠ¸ì‚¬ì´ë“œ í™˜ê²½ ë³€ìˆ˜ ì²´í¬
    openaiValid = openaiApiKey && openaiApiKey !== 'sk-your-openai-key-here' && openaiApiKey.startsWith('sk-')
    anthropicValid = anthropicApiKey && anthropicApiKey !== 'your-anthropic-key-here' && anthropicApiKey.startsWith('sk-ant-')
    googleValid = googleApiKey && googleApiKey !== 'your-google-ai-key-here'

    console.log(`- OpenAI: ${openaiValid ? 'âœ… ìœ íš¨' : 'âŒ ë¬´íš¨'} (ê¸¸ì´: ${openaiApiKey?.length || 0})`)
    console.log(`- Anthropic: ${anthropicValid ? 'âœ… ìœ íš¨' : 'âŒ ë¬´íš¨'} (ê¸¸ì´: ${anthropicApiKey?.length || 0})`)
    console.log(`- Google: ${googleValid ? 'âœ… ìœ íš¨' : 'âŒ ë¬´íš¨'} (ê¸¸ì´: ${googleApiKey?.length || 0})`)
  } else {
    // í”„ë¡œë•ì…˜ í™˜ê²½: ì„œë²„ì‚¬ì´ë“œ API Routesë¥¼ í†µí•´ ì‚¬ìš©í•˜ë¯€ë¡œ í•­ìƒ trueë¡œ ì„¤ì •
    openaiValid = true
    anthropicValid = true
    googleValid = true

    console.log(`- OpenAI: âœ… ì„œë²„ì‚¬ì´ë“œ API ì‚¬ìš© (API Routes: /api/openai)`)
    console.log(`- Anthropic: âœ… ì„œë²„ì‚¬ì´ë“œ API ì‚¬ìš© (API Routes: /api/anthropic)`)
    console.log(`- Google: âœ… ì„œë²„ì‚¬ì´ë“œ API ì‚¬ìš© (API Routes: /api/google)`)
  }

  // í™˜ê²½ ë³€ìˆ˜ ë””ë²„ê¹… ì •ë³´
  console.log('ğŸ“Š í™˜ê²½ ë³€ìˆ˜ ìƒíƒœ:')
  console.log('NODE_ENV:', import.meta.env['NODE_ENV'])
  console.log('MODE:', import.meta.env['MODE'])
  console.log('DEV:', import.meta.env['DEV'])
  console.log('PROD:', import.meta.env['PROD'])

  const defaultModels: AIModelConfig[] = []

  // OpenAI ëª¨ë¸ë“¤ (ìµœì‹  2024-2025 ëª¨ë¸ë“¤) - API í‚¤ ìœ íš¨ì„±ì— ê´€ê³„ì—†ì´ ë“±ë¡
  console.log('ğŸ“‹ OpenAI ëª¨ë¸ 5ê°œ ë“±ë¡ ì¤‘... (API í‚¤ ìƒíƒœ:', openaiValid ? 'âœ… ìœ íš¨' : 'âŒ ë¬´íš¨', ')')
  defaultModels.push(
      {
        id: 'gpt-4o',
        name: 'GPT-4o',
        provider: 'openai',
        model_id: 'gpt-4o',
        api_key: openaiApiKey,
        max_tokens: 128000,
        cost_per_input_token: 0.000005,
        cost_per_output_token: 0.000015,
        rate_limits: { requests_per_minute: 500, tokens_per_minute: 30000 }
      },
      {
        id: 'gpt-4o-mini',
        name: 'GPT-4o Mini',
        provider: 'openai',
        model_id: 'gpt-4o-mini',
        api_key: openaiApiKey,
        max_tokens: 128000,
        cost_per_input_token: 0.00000015,
        cost_per_output_token: 0.0000006,
        rate_limits: { requests_per_minute: 500, tokens_per_minute: 200000 }
      },
      {
        id: 'gpt-4-turbo',
        name: 'GPT-4 Turbo',
        provider: 'openai',
        model_id: 'gpt-4-turbo',
        api_key: openaiApiKey,
        max_tokens: 4096,
        cost_per_input_token: 0.00001,
        cost_per_output_token: 0.00003,
        rate_limits: { requests_per_minute: 500, tokens_per_minute: 30000 }
      },
      {
        id: 'gpt-3.5-turbo',
        name: 'GPT-3.5 Turbo',
        provider: 'openai',
        model_id: 'gpt-3.5-turbo',
        api_key: openaiApiKey,
        max_tokens: 4096,
        cost_per_input_token: 0.0000015,
        cost_per_output_token: 0.000002,
        rate_limits: { requests_per_minute: 3500, tokens_per_minute: 90000 }
      },
      {
        id: 'o1-preview',
        name: 'o1 Preview',
        provider: 'openai',
        model_id: 'o1-preview',
        api_key: openaiApiKey,
        max_tokens: 32768,
        cost_per_input_token: 0.000015,
        cost_per_output_token: 0.00006,
        rate_limits: { requests_per_minute: 20, tokens_per_minute: 20000 }
      }
    )

  // Anthropic ëª¨ë¸ë“¤ (ìµœì‹  Claude 4 ì‹œë¦¬ì¦ˆ í¬í•¨) - API í‚¤ ìœ íš¨ì„±ì— ê´€ê³„ì—†ì´ ë“±ë¡
  console.log('ğŸ“‹ Anthropic ëª¨ë¸ 7ê°œ ë“±ë¡ ì¤‘... (API í‚¤ ìƒíƒœ:', anthropicValid ? 'âœ… ìœ íš¨' : 'âŒ ë¬´íš¨', ')')
  defaultModels.push(
      {
        id: 'claude-opus-4-1',
        name: 'Claude Opus 4.1',
        provider: 'anthropic',
        model_id: 'claude-opus-4-1-20250805',
        api_key: anthropicApiKey,
        max_tokens: 8192,
        cost_per_input_token: 0.000015,
        cost_per_output_token: 0.000075,
        rate_limits: { requests_per_minute: 50, tokens_per_minute: 40000 }
      },
      {
        id: 'claude-opus-4',
        name: 'Claude Opus 4',
        provider: 'anthropic',
        model_id: 'claude-opus-4-20250514',
        api_key: anthropicApiKey,
        max_tokens: 8192,
        cost_per_input_token: 0.000015,
        cost_per_output_token: 0.000075,
        rate_limits: { requests_per_minute: 50, tokens_per_minute: 40000 }
      },
      {
        id: 'claude-sonnet-4',
        name: 'Claude Sonnet 4',
        provider: 'anthropic',
        model_id: 'claude-sonnet-4-20250514',
        api_key: anthropicApiKey,
        max_tokens: 8192,
        cost_per_input_token: 0.000003,
        cost_per_output_token: 0.000015,
        rate_limits: { requests_per_minute: 100, tokens_per_minute: 80000 }
      },
      {
        id: 'claude-3-7-sonnet',
        name: 'Claude 3.7 Sonnet',
        provider: 'anthropic',
        model_id: 'claude-3-7-sonnet-20250219',
        api_key: anthropicApiKey,
        max_tokens: 8192,
        cost_per_input_token: 0.000003,
        cost_per_output_token: 0.000015,
        rate_limits: { requests_per_minute: 100, tokens_per_minute: 80000 }
      },
      {
        id: 'claude-3-5-haiku',
        name: 'Claude 3.5 Haiku',
        provider: 'anthropic',
        model_id: 'claude-3-5-haiku-20241022',
        api_key: anthropicApiKey,
        max_tokens: 8192,
        cost_per_input_token: 0.0000008,
        cost_per_output_token: 0.000004,
        rate_limits: { requests_per_minute: 300, tokens_per_minute: 50000 }
      },
      {
        id: 'claude-3-opus',
        name: 'Claude 3 Opus',
        provider: 'anthropic',
        model_id: 'claude-3-opus-20240229',
        api_key: anthropicApiKey,
        max_tokens: 4096,
        cost_per_input_token: 0.000015,
        cost_per_output_token: 0.000075,
        rate_limits: { requests_per_minute: 100, tokens_per_minute: 10000 }
      },
      {
        id: 'claude-3-haiku',
        name: 'Claude 3 Haiku',
        provider: 'anthropic',
        model_id: 'claude-3-haiku-20240307',
        api_key: anthropicApiKey,
        max_tokens: 4096,
        cost_per_input_token: 0.00000025,
        cost_per_output_token: 0.00000125,
        rate_limits: { requests_per_minute: 300, tokens_per_minute: 50000 }
      }
    )

  // Google ëª¨ë¸ë“¤ (ìµœì‹  Gemini 2.0 ë° 2.5 ì‹œë¦¬ì¦ˆ) - API í‚¤ ìœ íš¨ì„±ì— ê´€ê³„ì—†ì´ ë“±ë¡
  console.log('ğŸ“‹ Google ëª¨ë¸ 5ê°œ ë“±ë¡ ì¤‘... (API í‚¤ ìƒíƒœ:', googleValid ? 'âœ… ìœ íš¨' : 'âŒ ë¬´íš¨', ')')
  defaultModels.push(
      {
        id: 'gemini-2.0-flash',
        name: 'Gemini 2.0 Flash',
        provider: 'google',
        model_id: 'gemini-2.0-flash',
        api_key: googleApiKey,
        max_tokens: 8192,
        cost_per_input_token: 0.000000125,
        cost_per_output_token: 0.000000375,
        rate_limits: { requests_per_minute: 1000, tokens_per_minute: 1000000 }
      },
      {
        id: 'gemini-2.5-pro-preview-tts',
        name: 'Gemini 2.5 Pro Preview TTS',
        provider: 'google',
        model_id: 'gemini-2.5-pro-preview-tts',
        api_key: googleApiKey,
        max_tokens: 8192,
        cost_per_input_token: 0.00000125,
        cost_per_output_token: 0.00000375,
        rate_limits: { requests_per_minute: 20, tokens_per_minute: 32000 }
      },
      {
        id: 'gemini-1.5-pro',
        name: 'Gemini 1.5 Pro',
        provider: 'google',
        model_id: 'gemini-1.5-pro',
        api_key: googleApiKey,
        max_tokens: 8192,
        cost_per_input_token: 0.00000125,
        cost_per_output_token: 0.00000375,
        rate_limits: { requests_per_minute: 360, tokens_per_minute: 10000000 }
      },
      {
        id: 'gemini-1.5-flash',
        name: 'Gemini 1.5 Flash',
        provider: 'google',
        model_id: 'gemini-1.5-flash',
        api_key: googleApiKey,
        max_tokens: 8192,
        cost_per_input_token: 0.000000075,
        cost_per_output_token: 0.0000003,
        rate_limits: { requests_per_minute: 1000, tokens_per_minute: 1000000 }
      },
      {
        id: 'gemini-pro',
        name: 'Gemini Pro',
        provider: 'google',
        model_id: 'gemini-pro',
        api_key: googleApiKey,
        max_tokens: 2048,
        cost_per_input_token: 0.0000005,
        cost_per_output_token: 0.0000015,
        rate_limits: { requests_per_minute: 60, tokens_per_minute: 32000 }
      }
    )

  console.log('ğŸ“Š ëª¨ë¸ ë“±ë¡ ê²°ê³¼:')
  console.log('- ì´ ìˆ˜ì§‘ëœ ëª¨ë¸ ìˆ˜:', defaultModels.length)
  console.log('- ìˆ˜ì§‘ëœ ëª¨ë¸ ëª©ë¡:', defaultModels.map(m => m.id))

  // ëª¨ë“  ëª¨ë¸ì´ API í‚¤ ìœ íš¨ì„±ì— ê´€ê³„ì—†ì´ ë“±ë¡ë¨

  console.log('ğŸ”§ AI Provider Factoryì— ëª¨ë¸ ë“±ë¡ ì¤‘...')
  defaultModels.forEach((model, index) => {
    console.log(`${index + 1}. ë“±ë¡ ì¤‘: ${model.id} (${model.provider})`)
    AIProviderFactory.registerModel(model)
  })

  // ë“±ë¡ ì™„ë£Œ í›„ í™•ì¸
  const registeredModels = AIProviderFactory.getRegisteredModels()
  console.log('âœ… ë“±ë¡ ì™„ë£Œëœ ëª¨ë¸ ìˆ˜:', registeredModels.length)
  console.log('âœ… ë“±ë¡ëœ ëª¨ë¸ ID:', registeredModels.map(m => m.id))

  // í´ë°± ì²´ì¸ ì„¤ì • (ë“±ë¡ëœ ëª¨ë¸ë“¤ë¡œë§Œ êµ¬ì„±)
  const availableModelIds = defaultModels.map(model => model.id)
  AIProviderFactory.setFallbackConfig({
    enabled: true,
    models: availableModelIds,
    max_retries: 3,
    retry_delay: 1000
  })

  console.log(`ğŸ¯ AI Provider Factory ì´ˆê¸°í™” ì™„ë£Œ: ${registeredModels.length}ê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥`)
  console.log('ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:', availableModelIds)
}

// íŒ©í† ë¦¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê¸°ë³¸ ë‚´ë³´ë‚´ê¸°
export const aiProviderFactory = AIProviderFactory