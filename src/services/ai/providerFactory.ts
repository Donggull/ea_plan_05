import { ApiUsageService } from '../apiUsageService'
import { RateLimiter } from '../../lib/rateLimiter'

// ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ë“¤ì„ staticìœ¼ë¡œ ì‚¬ìš©

// AI ì œê³µì—…ì²´ íƒ€ì… ì •ì˜
export type AIProvider = 'openai' | 'anthropic' | 'google' | 'custom'

// AI ìš”ì²­ ë©”ì‹œì§€ íƒ€ì…
export interface AIMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

// AI ëª¨ë¸ ì„¤ì • ì¸í„°í˜ì´ìŠ¤
export interface AIModelConfig {
  id: string
  name: string
  provider: AIProvider
  model_id: string
  api_key?: string
  api_endpoint?: string
  max_tokens: number
  temperature?: number
  top_p?: number
  cost_per_input_token: number
  cost_per_output_token: number
  rate_limits: {
    requests_per_minute: number
    tokens_per_minute: number
  }
}

// AI ì‘ë‹µ ì¸í„°í˜ì´ìŠ¤
export interface AIResponse {
  content: string
  model: string
  usage: {
    input_tokens: number
    output_tokens: number
    total_tokens: number
  }
  cost: number
  response_time: number
  finish_reason: 'stop' | 'length' | 'error'
}

// AI ìš”ì²­ ì˜µì…˜
export interface AIRequestOptions {
  messages: AIMessage[]
  max_tokens?: number
  temperature?: number
  top_p?: number
  stream?: boolean
  user_id?: string
}

// í´ë°± ì„¤ì •
export interface FallbackConfig {
  enabled: boolean
  models: string[] // ìš°ì„ ìˆœìœ„ëŒ€ë¡œ ì •ë ¬ëœ ëª¨ë¸ ID ëª©ë¡
  max_retries: number
  retry_delay: number // ms
}

// ì—ëŸ¬ íƒ€ì…
export class AIProviderError extends Error {
  constructor(
    message: string,
    public provider: AIProvider,
    public model: string,
    public statusCode?: number,
    public retryable: boolean = false
  ) {
    super(message)
    this.name = 'AIProviderError'
  }
}

// ê¸°ë³¸ AI ì œê³µì—…ì²´ í´ë˜ìŠ¤
abstract class BaseAIProvider {
  protected config: AIModelConfig

  constructor(config: AIModelConfig) {
    this.config = config
  }

  abstract generateCompletion(options: AIRequestOptions): Promise<AIResponse>

  // ê³µí†µ í† í° ê³„ì‚° ë©”ì„œë“œ
  protected estimateTokens(text: string): number {
    // ê°„ë‹¨í•œ í† í° ì¶”ì • - ì‹¤ì œë¡œëŠ” ëª¨ë¸ë³„ í† í¬ë‚˜ì´ì € ì‚¬ìš©
    return Math.ceil(text.length / 4)
  }

  // ë¹„ìš© ê³„ì‚°
  protected calculateCost(inputTokens: number, outputTokens: number): number {
    return (inputTokens * this.config.cost_per_input_token) +
           (outputTokens * this.config.cost_per_output_token)
  }

  // Rate Limiting ì²´í¬
  protected async checkRateLimit(userId: string): Promise<void> {
    const allowed = await RateLimiter.checkRateLimit(userId, 'user', 1)
    if (!allowed.allowed) {
      throw new AIProviderError(
        `Rate limit exceeded: ${allowed.reason}`,
        this.config.provider,
        this.config.model_id,
        429,
        true
      )
    }
  }
}

// OpenAI ì œê³µì—…ì²´
class OpenAIProvider extends BaseAIProvider {
  async generateCompletion(options: AIRequestOptions): Promise<AIResponse> {
    const startTime = Date.now()

    try {
      // Rate Limiting ì²´í¬
      if (options.user_id) {
        await this.checkRateLimit(options.user_id)
      }

      // API í‚¤ í™•ì¸ - ë°˜ë“œì‹œ ì‹¤ì œ API í‚¤ê°€ ìˆì–´ì•¼ í•¨
      if (!this.config.api_key || this.config.api_key === 'sk-your-openai-key-here' || !this.config.api_key.startsWith('sk-')) {
        console.error('âŒ OpenAI API í‚¤ ì˜¤ë¥˜:', {
          hasKey: !!this.config.api_key,
          keyPrefix: this.config.api_key?.substring(0, 10),
          keyLength: this.config.api_key?.length
        })
        throw new AIProviderError(
          'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ VITE_OPENAI_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.',
          'openai',
          this.config.model_id,
          401,
          false
        )
      }

      return await this.callOpenAIAPI(options, startTime)

    } catch (error) {
      if (error instanceof AIProviderError) {
        throw error
      }
      throw new AIProviderError(
        `OpenAI API Error: ${error}`,
        'openai',
        this.config.model_id,
        500,
        true
      )
    }
  }

  private async callOpenAIAPI(options: AIRequestOptions, startTime: number): Promise<AIResponse> {
    // ê°œë°œ í™˜ê²½ê³¼ Vercel í™˜ê²½ ëª¨ë‘ í”„ë¡ì‹œ/API Routes ì‚¬ìš©
    const apiUrl = '/api/openai/v1/chat/completions'

    console.log('ğŸŒ OpenAI API URL:', apiUrl, '(dev mode:', import.meta.env.DEV, ')')

    const response = await fetch(apiUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.config.api_key}`
      },
      body: JSON.stringify({
        model: this.config.model_id,
        messages: options.messages,
        max_tokens: options.max_tokens || this.config.max_tokens,
        temperature: options.temperature || 0.7,
        top_p: options.top_p || 1.0
      })
    })

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}))
      throw new Error(`HTTP ${response.status}: ${errorData.error?.message || response.statusText}`)
    }

    const data = await response.json()
    const usage = data.usage || {}
    const cost = this.calculateCost(usage.prompt_tokens || 0, usage.completion_tokens || 0)

    const aiResponse: AIResponse = {
      content: data.choices?.[0]?.message?.content || 'No response content',
      model: this.config.model_id,
      usage: {
        input_tokens: usage.prompt_tokens || 0,
        output_tokens: usage.completion_tokens || 0,
        total_tokens: usage.total_tokens || 0
      },
      cost,
      response_time: Date.now() - startTime,
      finish_reason: data.choices?.[0]?.finish_reason || 'stop'
    }

    // ì‚¬ìš©ëŸ‰ ê¸°ë¡
    if (options.user_id) {
      await ApiUsageService.recordUsageBatch([{
        userId: options.user_id!,
        model: this.config.model_id,
        inputTokens: aiResponse.usage.input_tokens,
        outputTokens: aiResponse.usage.output_tokens,
        cost: aiResponse.cost
      }])
    }

    return aiResponse
  }

}

// Anthropic ì œê³µì—…ì²´
class AnthropicProvider extends BaseAIProvider {
  async generateCompletion(options: AIRequestOptions): Promise<AIResponse> {
    const startTime = Date.now()

    try {
      // Rate Limiting ì²´í¬
      if (options.user_id) {
        await this.checkRateLimit(options.user_id)
      }

      // API í‚¤ í™•ì¸ - ë°˜ë“œì‹œ ì‹¤ì œ API í‚¤ê°€ ìˆì–´ì•¼ í•¨
      if (!this.config.api_key || this.config.api_key === 'your-anthropic-key-here') {
        throw new AIProviderError(
          'Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ VITE_ANTHROPIC_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.',
          'anthropic',
          this.config.model_id,
          401,
          false
        )
      }

      return await this.callAnthropicAPI(options, startTime)

    } catch (error) {
      if (error instanceof AIProviderError) {
        throw error
      }
      throw new AIProviderError(
        `Anthropic API Error: ${error}`,
        'anthropic',
        this.config.model_id,
        500,
        true
      )
    }
  }

  private async callAnthropicAPI(options: AIRequestOptions, startTime: number): Promise<AIResponse> {
    try {
      // Anthropic API ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜ - ë” ì—„ê²©í•œ ìœ íš¨ì„± ê²€ì‚¬
      const anthropicMessages = options.messages
        .filter(m => m.role !== 'system' && m.content && m.content.trim())
        .map(m => ({
          role: m.role === 'user' ? 'user' as const : 'assistant' as const,
          content: m.content.trim()
        }))

      // ìµœì†Œ 1ê°œì˜ ë©”ì‹œì§€ê°€ í•„ìš”í•¨
      if (anthropicMessages.length === 0) {
        throw new Error('ìµœì†Œ 1ê°œì˜ ìœ íš¨í•œ ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.')
      }

      const systemMessage = options.messages.find(m => m.role === 'system')?.content?.trim() || ''

      const requestBody = {
        model: this.config.model_id,
        max_tokens: Math.min(options.max_tokens || this.config.max_tokens, 4096),
        temperature: Math.max(0, Math.min(1, options.temperature || 0.7)),
        messages: anthropicMessages,
        ...(systemMessage && { system: systemMessage })
      }

      console.log('ğŸ” Anthropic API ìš”ì²­ ì„¸ë¶€ì •ë³´:', {
        url: 'https://api.anthropic.com/v1/messages',
        method: 'POST',
        model: this.config.model_id,
        messageCount: anthropicMessages.length,
        hasSystem: !!systemMessage,
        apiKeyPrefix: this.config.api_key?.substring(0, 10) + '...',
        apiKeyLength: this.config.api_key?.length,
        requestBodySize: JSON.stringify(requestBody).length
      })

      // Anthropic API í‚¤ ìœ íš¨ì„± ì¬í™•ì¸
      if (!this.config.api_key || this.config.api_key === 'your-anthropic-key-here' || !this.config.api_key.startsWith('sk-ant-')) {
        throw new Error(`ì˜ëª»ëœ Anthropic API í‚¤ì…ë‹ˆë‹¤. í‚¤ í˜•ì‹: ${this.config.api_key?.substring(0, 10)}...`)
      }

      // ê°œë°œ í™˜ê²½ê³¼ Vercel í™˜ê²½ ëª¨ë‘ í”„ë¡ì‹œ/API Routes ì‚¬ìš©
      const apiUrl = '/api/anthropic/v1/messages'

      console.log('ğŸŒ API URL:', apiUrl, '(dev mode:', import.meta.env.DEV, ')')

      const response = await fetch(apiUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-api-key': this.config.api_key!,
          'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify(requestBody)
      }).catch(fetchError => {
        console.error('ğŸš¨ Fetch ì˜¤ë¥˜ ìƒì„¸:', fetchError)
        throw fetchError
      })

      console.log('ğŸ“¡ Anthropic API ì‘ë‹µ ìƒíƒœ:', {
        status: response.status,
        statusText: response.statusText,
        ok: response.ok,
        headers: Object.fromEntries(response.headers.entries())
      })

      if (!response.ok) {
        const errorText = await response.text()
        console.error('âŒ Anthropic API ì—ëŸ¬ ì‘ë‹µ:', errorText)

        let errorData
        try {
          errorData = JSON.parse(errorText)
        } catch {
          errorData = { error: { message: errorText } }
        }

        // êµ¬ì²´ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±
        const errorMessage = errorData.error?.message ||
                           errorData.message ||
                           `HTTP ${response.status}: ${response.statusText}`

        throw new Error(errorMessage)
      }

      const data = await response.json()
      console.log('âœ… Anthropic API ì„±ê³µ ì‘ë‹µ:', {
        contentLength: data.content?.[0]?.text?.length || 0,
        usage: data.usage,
        stopReason: data.stop_reason
      })

      const usage = data.usage || {}
      const cost = this.calculateCost(usage.input_tokens || 0, usage.output_tokens || 0)

      const aiResponse: AIResponse = {
        content: data.content?.[0]?.text || 'No response content',
        model: this.config.model_id,
        usage: {
          input_tokens: usage.input_tokens || 0,
          output_tokens: usage.output_tokens || 0,
          total_tokens: (usage.input_tokens || 0) + (usage.output_tokens || 0)
        },
        cost,
        response_time: Date.now() - startTime,
        finish_reason: data.stop_reason === 'end_turn' ? 'stop' : data.stop_reason || 'stop'
      }

      // ì‚¬ìš©ëŸ‰ ê¸°ë¡
      if (options.user_id) {
        await ApiUsageService.recordUsageBatch([{
          userId: options.user_id!,
          model: this.config.model_id,
          inputTokens: aiResponse.usage.input_tokens,
          outputTokens: aiResponse.usage.output_tokens,
          cost: aiResponse.cost
        }])
      }

      return aiResponse

    } catch (error) {
      console.error('ğŸš¨ Anthropic API í˜¸ì¶œ ì‹¤íŒ¨ - ìƒì„¸ ì§„ë‹¨:', {
        error: error,
        errorMessage: error instanceof Error ? error.message : String(error),
        errorName: error instanceof Error ? error.name : 'Unknown',
        errorStack: error instanceof Error ? error.stack : undefined,
        apiKey: this.config.api_key?.substring(0, 15) + '...',
        modelId: this.config.model_id
      })

      // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
      if (error instanceof TypeError && error.message.includes('Failed to fetch')) {
        throw new Error('ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜: Anthropic APIì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì´ë‚˜ ë°©í™”ë²½ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.')
      }

      // CORS ì˜¤ë¥˜ì¸ì§€ í™•ì¸
      if (error instanceof Error && error.message.includes('CORS')) {
        throw new Error('CORS ì˜¤ë¥˜: ë¸Œë¼ìš°ì €ì—ì„œ Anthropic APIì— ì§ì ‘ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡ì‹œ ì„œë²„ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.')
      }

      throw error
    }
  }

}

// Google ì œê³µì—…ì²´
class GoogleProvider extends BaseAIProvider {
  async generateCompletion(options: AIRequestOptions): Promise<AIResponse> {
    const startTime = Date.now()

    try {
      // Rate Limiting ì²´í¬
      if (options.user_id) {
        await this.checkRateLimit(options.user_id)
      }

      // API í‚¤ í™•ì¸ - ë°˜ë“œì‹œ ì‹¤ì œ API í‚¤ê°€ ìˆì–´ì•¼ í•¨
      if (!this.config.api_key || this.config.api_key === 'your-google-ai-key-here') {
        console.error('âŒ Google AI API í‚¤ ì˜¤ë¥˜:', {
          hasKey: !!this.config.api_key,
          keyPrefix: this.config.api_key?.substring(0, 10),
          keyLength: this.config.api_key?.length
        })
        throw new AIProviderError(
          'Google AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ VITE_GOOGLE_AI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.',
          'google',
          this.config.model_id,
          401,
          false
        )
      }

      return await this.callGoogleAIAPI(options, startTime)

    } catch (error) {
      if (error instanceof AIProviderError) {
        throw error
      }
      throw new AIProviderError(
        `Google AI API Error: ${error}`,
        'google',
        this.config.model_id,
        500,
        true
      )
    }
  }

  private async callGoogleAIAPI(options: AIRequestOptions, startTime: number): Promise<AIResponse> {
    // Google AI API ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜
    const parts = options.messages
      .filter(m => m.role !== 'system')
      .map(m => ({ text: m.content }))

    const systemMessage = options.messages.find(m => m.role === 'system')?.content

    const requestBody = {
      contents: [{
        parts: parts
      }],
      generationConfig: {
        temperature: options.temperature || 0.7,
        topP: options.top_p || 1.0,
        maxOutputTokens: options.max_tokens || this.config.max_tokens,
      }
    }

    // ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if (systemMessage) {
      requestBody.contents.unshift({
        parts: [{ text: systemMessage }]
      })
    }

    // ê°œë°œ í™˜ê²½ê³¼ Vercel í™˜ê²½ ëª¨ë‘ í”„ë¡ì‹œ/API Routes ì‚¬ìš©
    const apiUrl = `/api/google/v1beta/models/${this.config.model_id}:generateContent?key=${this.config.api_key}`

    console.log('ğŸŒ Google AI API URL:', apiUrl, '(dev mode:', import.meta.env.DEV, ')')

    const response = await fetch(apiUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(requestBody)
    })

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}))
      throw new Error(`HTTP ${response.status}: ${errorData.error?.message || response.statusText}`)
    }

    const data = await response.json()

    // Google AI API ì‘ë‹µ êµ¬ì¡° ì²˜ë¦¬
    const content = data.candidates?.[0]?.content?.parts?.[0]?.text || 'No response content'
    const inputTokens = data.usageMetadata?.promptTokenCount || 0
    const outputTokens = data.usageMetadata?.candidatesTokenCount || 0
    const cost = this.calculateCost(inputTokens, outputTokens)

    const aiResponse: AIResponse = {
      content,
      model: this.config.model_id,
      usage: {
        input_tokens: inputTokens,
        output_tokens: outputTokens,
        total_tokens: inputTokens + outputTokens
      },
      cost,
      response_time: Date.now() - startTime,
      finish_reason: data.candidates?.[0]?.finishReason === 'STOP' ? 'stop' : 'length'
    }

    // ì‚¬ìš©ëŸ‰ ê¸°ë¡
    if (options.user_id) {
      await ApiUsageService.recordUsageBatch([{
        userId: options.user_id!,
        model: this.config.model_id,
        inputTokens: aiResponse.usage.input_tokens,
        outputTokens: aiResponse.usage.output_tokens,
        cost: aiResponse.cost
      }])
    }

    return aiResponse
  }
}

// ì»¤ìŠ¤í…€ ì œê³µì—…ì²´
class CustomProvider extends BaseAIProvider {
  async generateCompletion(options: AIRequestOptions): Promise<AIResponse> {
    const startTime = Date.now()

    try {
      // Rate Limiting ì²´í¬
      if (options.user_id) {
        await this.checkRateLimit(options.user_id)
      }

      // ì»¤ìŠ¤í…€ API ìš”ì²­
      const response = await fetch(this.config.api_endpoint || '', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.config.api_key}`
        },
        body: JSON.stringify({
          model: this.config.model_id,
          messages: options.messages,
          max_tokens: options.max_tokens || this.config.max_tokens,
          temperature: options.temperature || 0.7
        })
      })

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`)
      }

      const data = await response.json()
      const inputText = options.messages.map(m => m.content).join(' ')
      const inputTokens = this.estimateTokens(inputText)
      const outputTokens = this.estimateTokens(data.content || '')

      const aiResponse: AIResponse = {
        content: data.content || data.message || 'Custom API response',
        model: this.config.model_id,
        usage: {
          input_tokens: inputTokens,
          output_tokens: outputTokens,
          total_tokens: inputTokens + outputTokens
        },
        cost: this.calculateCost(inputTokens, outputTokens),
        response_time: Date.now() - startTime,
        finish_reason: 'stop'
      }

      // ì‚¬ìš©ëŸ‰ ê¸°ë¡
      if (options.user_id) {
        const cost = this.calculateCost(inputTokens, outputTokens)
        await ApiUsageService.recordUsageBatch([{
          userId: options.user_id!,
          model: this.config.model_id,
          inputTokens: inputTokens,
          outputTokens: outputTokens,
          cost
        }])
      }

      return aiResponse

    } catch (error) {
      throw new AIProviderError(
        `Custom API Error: ${error}`,
        'custom',
        this.config.model_id,
        500,
        true
      )
    }
  }
}

// AI ì œê³µì—…ì²´ íŒ©í† ë¦¬ í´ë˜ìŠ¤
export class AIProviderFactory {
  private static models: Map<string, AIModelConfig> = new Map()
  private static providers: Map<string, BaseAIProvider> = new Map()
  private static fallbackConfig: FallbackConfig = {
    enabled: true,
    models: [],
    max_retries: 3,
    retry_delay: 1000
  }

  // ëª¨ë¸ ë“±ë¡
  static registerModel(config: AIModelConfig): void {
    this.models.set(config.id, config)

    // ì œê³µì—…ì²´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    let provider: BaseAIProvider
    switch (config.provider) {
      case 'openai':
        provider = new OpenAIProvider(config)
        break
      case 'anthropic':
        provider = new AnthropicProvider(config)
        break
      case 'google':
        provider = new GoogleProvider(config)
        break
      case 'custom':
        provider = new CustomProvider(config)
        break
      default:
        throw new Error(`Unsupported provider: ${config.provider}`)
    }

    this.providers.set(config.id, provider)
  }

  // ëª¨ë¸ ì œê±°
  static unregisterModel(modelId: string): void {
    this.models.delete(modelId)
    this.providers.delete(modelId)
  }

  // ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
  static getRegisteredModels(): AIModelConfig[] {
    return Array.from(this.models.values())
  }

  // íŠ¹ì • ëª¨ë¸ ì¡°íšŒ
  static getModel(modelId: string): AIModelConfig | undefined {
    return this.models.get(modelId)
  }

  // í´ë°± ì„¤ì •
  static setFallbackConfig(config: Partial<FallbackConfig>): void {
    this.fallbackConfig = { ...this.fallbackConfig, ...config }
  }

  // AI ì™„ë£Œ ìš”ì²­ (í´ë°± ì§€ì›)
  static async generateCompletion(
    modelId: string,
    options: AIRequestOptions
  ): Promise<AIResponse> {
    const modelsToTry = this.fallbackConfig.enabled
      ? [modelId, ...this.fallbackConfig.models.filter(id => id !== modelId)]
      : [modelId]

    let lastError: Error | null = null

    for (let i = 0; i < modelsToTry.length && i < this.fallbackConfig.max_retries; i++) {
      const currentModelId = modelsToTry[i]
      const provider = this.providers.get(currentModelId)

      if (!provider) {
        lastError = new Error(`Model not found: ${currentModelId}`)
        continue
      }

      try {
        console.log(`Attempting to use model: ${currentModelId}`)
        const response = await provider.generateCompletion(options)

        if (i > 0) {
          console.log(`Fallback successful with model: ${currentModelId}`)
        }

        return response

      } catch (error) {
        lastError = error as Error
        console.warn(`Model ${currentModelId} failed:`, error)

        // ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬ê°€ ì•„ë‹ˆë©´ ì¦‰ì‹œ ì¤‘ë‹¨
        if (error instanceof AIProviderError && !error.retryable) {
          break
        }

        // ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì ì‹œ ëŒ€ê¸°
        if (i < modelsToTry.length - 1) {
          await new Promise(resolve =>
            setTimeout(resolve, this.fallbackConfig.retry_delay * (i + 1))
          )
        }
      }
    }

    // ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
    throw lastError || new Error('All AI providers failed')
  }

  // í—¬ìŠ¤ ì²´í¬
  static async healthCheck(modelId?: string): Promise<Record<string, boolean>> {
    const modelsToCheck = modelId ? [modelId] : Array.from(this.models.keys())
    const results: Record<string, boolean> = {}

    await Promise.allSettled(
      modelsToCheck.map(async (id) => {
        try {
          const provider = this.providers.get(id)
          if (!provider) {
            results[id] = false
            return
          }

          // ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
          await provider.generateCompletion({
            messages: [{ role: 'user', content: 'test' }],
            max_tokens: 10
          })
          results[id] = true
        } catch {
          results[id] = false
        }
      })
    )

    return results
  }

  // ì œê³µì—…ì²´ë³„ í†µê³„
  static getProviderStats(): Record<AIProvider, { models: number; active: number }> {
    const stats: Record<AIProvider, { models: number; active: number }> = {
      openai: { models: 0, active: 0 },
      anthropic: { models: 0, active: 0 },
      google: { models: 0, active: 0 },
      custom: { models: 0, active: 0 }
    }

    this.models.forEach(model => {
      stats[model.provider].models++
      if (this.providers.has(model.id)) {
        stats[model.provider].active++
      }
    })

    return stats
  }

  // ëª¨ë“  ì œê³µì—…ì²´ ì´ˆê¸°í™”
  static clear(): void {
    this.models.clear()
    this.providers.clear()
  }
}

// ê¸°ë³¸ ëª¨ë¸ë“¤ ë“±ë¡
export function initializeDefaultModels(): void {
  // í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸°
  const openaiApiKey = import.meta.env.VITE_OPENAI_API_KEY
  const anthropicApiKey = import.meta.env.VITE_ANTHROPIC_API_KEY
  const googleApiKey = import.meta.env.VITE_GOOGLE_AI_API_KEY

  console.log('ğŸ”‘ AI API í‚¤ ìƒì„¸ í™•ì¸:')
  console.log('OpenAI:', {
    present: !!openaiApiKey,
    valid: openaiApiKey && openaiApiKey !== 'sk-your-openai-key-here' && openaiApiKey.startsWith('sk-'),
    prefix: openaiApiKey?.substring(0, 7) + '...',
    length: openaiApiKey?.length
  })
  console.log('Anthropic:', {
    present: !!anthropicApiKey,
    valid: anthropicApiKey && anthropicApiKey !== 'your-anthropic-key-here' && anthropicApiKey.startsWith('sk-ant-'),
    prefix: anthropicApiKey?.substring(0, 10) + '...',
    length: anthropicApiKey?.length
  })
  console.log('Google:', {
    present: !!googleApiKey,
    valid: googleApiKey && googleApiKey !== 'your-google-ai-key-here',
    prefix: googleApiKey?.substring(0, 7) + '...',
    length: googleApiKey?.length
  })

  // í™˜ê²½ ë³€ìˆ˜ ë””ë²„ê¹… ì •ë³´
  console.log('ğŸ“Š í™˜ê²½ ë³€ìˆ˜ ìƒíƒœ:')
  console.log('NODE_ENV:', import.meta.env['NODE_ENV'])
  console.log('MODE:', import.meta.env['MODE'])
  console.log('DEV:', import.meta.env['DEV'])
  console.log('PROD:', import.meta.env['PROD'])

  const defaultModels: AIModelConfig[] = []

  // OpenAI ëª¨ë¸ë“¤ (API í‚¤ê°€ ìˆì„ ë•Œë§Œ ë“±ë¡)
  if (openaiApiKey && openaiApiKey !== 'sk-your-openai-key-here') {
    defaultModels.push(
      {
        id: 'gpt-4o',
        name: 'GPT-4o',
        provider: 'openai',
        model_id: 'gpt-4o',
        api_key: openaiApiKey,
        max_tokens: 128000,
        cost_per_input_token: 0.000005,
        cost_per_output_token: 0.000015,
        rate_limits: { requests_per_minute: 500, tokens_per_minute: 30000 }
      },
      {
        id: 'gpt-4-turbo',
        name: 'GPT-4 Turbo',
        provider: 'openai',
        model_id: 'gpt-4-turbo-preview',
        api_key: openaiApiKey,
        max_tokens: 4096,
        cost_per_input_token: 0.00001,
        cost_per_output_token: 0.00003,
        rate_limits: { requests_per_minute: 500, tokens_per_minute: 30000 }
      }
    )
  }

  // Anthropic ëª¨ë¸ë“¤ (API í‚¤ê°€ ìˆì„ ë•Œë§Œ ë“±ë¡)
  if (anthropicApiKey && anthropicApiKey !== 'your-anthropic-key-here') {
    defaultModels.push(
      {
        id: 'claude-3-opus',
        name: 'Claude 3 Opus',
        provider: 'anthropic',
        model_id: 'claude-3-opus-20240229',
        api_key: anthropicApiKey,
        max_tokens: 4096,
        cost_per_input_token: 0.000015,
        cost_per_output_token: 0.000075,
        rate_limits: { requests_per_minute: 100, tokens_per_minute: 10000 }
      },
      {
        id: 'claude-3-sonnet',
        name: 'Claude 3 Sonnet',
        provider: 'anthropic',
        model_id: 'claude-3-sonnet-20240229',
        api_key: anthropicApiKey,
        max_tokens: 4096,
        cost_per_input_token: 0.000003,
        cost_per_output_token: 0.000015,
        rate_limits: { requests_per_minute: 300, tokens_per_minute: 20000 }
      }
    )
  }

  // Google ëª¨ë¸ë“¤ (API í‚¤ê°€ ìˆì„ ë•Œë§Œ ë“±ë¡)
  if (googleApiKey && googleApiKey !== 'your-google-ai-key-here') {
    defaultModels.push({
      id: 'gemini-pro',
      name: 'Gemini Pro',
      provider: 'google',
      model_id: 'gemini-pro',
      api_key: googleApiKey,
      max_tokens: 2048,
      cost_per_input_token: 0.0000005,
      cost_per_output_token: 0.0000015,
      rate_limits: { requests_per_minute: 60, tokens_per_minute: 5000 }
    })
  }

  // ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì—ëŸ¬ ë©”ì‹œì§€
  if (defaultModels.length === 0) {
    console.error('âŒ AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
    console.error('ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ ì¤‘ í•˜ë‚˜ ì´ìƒì„ ì„¤ì •í•´ì£¼ì„¸ìš”:')
    console.error('- VITE_OPENAI_API_KEY: OpenAI API í‚¤')
    console.error('- VITE_ANTHROPIC_API_KEY: Anthropic API í‚¤')
    console.error('- VITE_GOOGLE_AI_API_KEY: Google AI API í‚¤')
    console.error('ì„¤ì • í›„ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë‹¤ì‹œ ì‹œì‘í•´ì£¼ì„¸ìš”.')
    // ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì•„ë¬´ê²ƒë„ ë“±ë¡í•˜ì§€ ì•ŠìŒ
    return
  }

  defaultModels.forEach(model => {
    AIProviderFactory.registerModel(model)
  })

  // í´ë°± ì²´ì¸ ì„¤ì • (ë“±ë¡ëœ ëª¨ë¸ë“¤ë¡œë§Œ êµ¬ì„±)
  const availableModelIds = defaultModels.map(model => model.id)
  AIProviderFactory.setFallbackConfig({
    enabled: true,
    models: availableModelIds,
    max_retries: 3,
    retry_delay: 1000
  })

  console.log(`âœ… ${defaultModels.length}ê°œì˜ AI ëª¨ë¸ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤:`, availableModelIds)
}

// íŒ©í† ë¦¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê¸°ë³¸ ë‚´ë³´ë‚´ê¸°
export const aiProviderFactory = AIProviderFactory