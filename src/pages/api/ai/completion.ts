import type { NextApiRequest, NextApiResponse } from 'next';
import OpenAI from 'openai';
import Anthropic from '@anthropic-ai/sdk';
import { GoogleGenerativeAI } from '@google/generative-ai';
import { contextCache } from '../../../services/preAnalysis/ContextCache';
import { promptEngine } from '../../../services/preAnalysis/PromptEngine';
import type { EnrichedContext } from '../../../services/preAnalysis/MCPAIBridge';

// AI í”„ë¡œë°”ì´ë” íƒ€ì…
type AIProvider = 'openai' | 'anthropic' | 'google';

// ìš”ì²­ íƒ€ì… (ì»¨í…ìŠ¤íŠ¸ ì§€ì› ì¶”ê°€)
interface AICompletionRequest {
  provider: AIProvider;
  model: string;
  messages: Array<{
    role: 'system' | 'user' | 'assistant';
    content: string;
  }>;
  temperature?: number;
  maxTokens?: number;
  stream?: boolean;
  // MCP ì»¨í…ìŠ¤íŠ¸ í†µí•© í•„ë“œ
  sessionId?: string;
  useContext?: boolean;
  analysisType?: 'project_analysis' | 'market_research' | 'tech_evaluation' | 'comprehensive';
  contextOptions?: {
    includeProjectStructure?: boolean;
    includeMarketAnalysis?: boolean;
    includeTechTrends?: boolean;
    forceRefresh?: boolean;
  };
}

// ì‘ë‹µ íƒ€ì… (ì»¨í…ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€)
interface AICompletionResponse {
  success: boolean;
  data?: {
    content: string;
    usage: {
      promptTokens: number;
      completionTokens: number;
      totalTokens: number;
    };
    model: string;
    finishReason?: string;
    // ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ ë©”íƒ€ë°ì´í„°
    contextUsed?: boolean;
    contextMetadata?: {
      sessionId: string;
      dataSourceCount: number;
      totalConfidence: number;
      lastUpdated: string;
      cacheHit?: boolean;
    };
  };
  error?: string;
}

// AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
const initializeClients = () => {
  const openai = process.env['OPENAI_API_KEY'] ? new OpenAI({
    apiKey: process.env['OPENAI_API_KEY'],
  }) : null;

  const anthropic = process.env['ANTHROPIC_API_KEY'] ? new Anthropic({
    apiKey: process.env['ANTHROPIC_API_KEY'],
  }) : null;

  const google = process.env['GOOGLE_AI_API_KEY'] ? new GoogleGenerativeAI(
    process.env['GOOGLE_AI_API_KEY']
  ) : null;

  return { openai, anthropic, google };
};

// OpenAI í˜¸ì¶œ
async function callOpenAI(
  client: OpenAI,
  request: AICompletionRequest
): Promise<AICompletionResponse> {
  try {
    const completion = await client.chat.completions.create({
      model: request.model,
      messages: request.messages,
      temperature: request.temperature || 0.7,
      max_tokens: request.maxTokens || 1000,
    });

    const choice = completion.choices[0];
    if (!choice || !choice.message?.content) {
      throw new Error('No response content from OpenAI');
    }

    return {
      success: true,
      data: {
        content: choice.message.content,
        usage: {
          promptTokens: completion.usage?.prompt_tokens || 0,
          completionTokens: completion.usage?.completion_tokens || 0,
          totalTokens: completion.usage?.total_tokens || 0,
        },
        model: completion.model,
        finishReason: choice.finish_reason || undefined,
      },
    };
  } catch (error) {
    console.error('OpenAI API error:', error);
    return {
      success: false,
      error: error instanceof Error ? error.message : 'OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤',
    };
  }
}

// Anthropic í˜¸ì¶œ
async function callAnthropic(
  client: Anthropic,
  request: AICompletionRequest
): Promise<AICompletionResponse> {
  try {
    const systemMessage = request.messages.find(m => m.role === 'system');
    const userMessages = request.messages.filter(m => m.role !== 'system');

    const message = await client.messages.create({
      model: request.model,
      max_tokens: request.maxTokens || 1000,
      temperature: request.temperature || 0.7,
      system: systemMessage?.content,
      messages: userMessages.map(m => ({
        role: m.role as 'user' | 'assistant',
        content: m.content,
      })),
    });

    const content = message.content[0];
    if (content.type !== 'text') {
      throw new Error('Unexpected response type from Anthropic');
    }

    return {
      success: true,
      data: {
        content: content.text,
        usage: {
          promptTokens: message.usage.input_tokens,
          completionTokens: message.usage.output_tokens,
          totalTokens: message.usage.input_tokens + message.usage.output_tokens,
        },
        model: message.model,
        finishReason: message.stop_reason || undefined,
      },
    };
  } catch (error) {
    console.error('Anthropic API error:', error);
    return {
      success: false,
      error: error instanceof Error ? error.message : 'Anthropic API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤',
    };
  }
}

// Google AI í˜¸ì¶œ
async function callGoogle(
  client: GoogleGenerativeAI,
  request: AICompletionRequest
): Promise<AICompletionResponse> {
  try {
    const model = client.getGenerativeModel({ model: request.model });

    // ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê²°í•©
    const systemMessage = request.messages.find(m => m.role === 'system');
    const userMessages = request.messages.filter(m => m.role !== 'system');

    let prompt = '';
    if (systemMessage) {
      prompt += `ì‹œìŠ¤í…œ ì§€ì¹¨: ${systemMessage.content}\n\n`;
    }

    userMessages.forEach(msg => {
      prompt += `${msg.role === 'user' ? 'ì‚¬ìš©ì' : 'ì–´ì‹œìŠ¤í„´íŠ¸'}: ${msg.content}\n`;
    });

    const result = await model.generateContent({
      contents: [{ role: 'user', parts: [{ text: prompt }] }],
      generationConfig: {
        temperature: request.temperature || 0.7,
        maxOutputTokens: request.maxTokens || 1000,
      },
    });

    const response = result.response;
    const content = response.text();

    if (!content) {
      throw new Error('No response content from Google AI');
    }

    // Google AIëŠ” ìƒì„¸í•œ ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì¶”ì •ê°’ ì‚¬ìš©
    const estimatedTokens = Math.ceil(content.length / 4);

    return {
      success: true,
      data: {
        content,
        usage: {
          promptTokens: Math.ceil(prompt.length / 4),
          completionTokens: estimatedTokens,
          totalTokens: Math.ceil(prompt.length / 4) + estimatedTokens,
        },
        model: request.model,
        finishReason: 'stop',
      },
    };
  } catch (error) {
    console.error('Google AI API error:', error);
    return {
      success: false,
      error: error instanceof Error ? error.message : 'Google AI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤',
    };
  }
}

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse<AICompletionResponse>
) {
  // CORS í—¤ë” ì„¤ì •
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type, Authorization');

  if (req.method === 'OPTIONS') {
    res.status(200).end();
    return;
  }

  if (req.method !== 'POST') {
    return res.status(405).json({
      success: false,
      error: 'Method not allowed. Use POST.',
    });
  }

  try {
    const request: AICompletionRequest = req.body;

    // API í‚¤ ìƒíƒœ ë¡œê¹… (ë””ë²„ê¹…ìš©)
    console.log('ğŸ” API í‚¤ ìƒíƒœ í™•ì¸:', {
      openaiKey: process.env['OPENAI_API_KEY'] ? 'ì„¤ì •ë¨' : 'ë¯¸ì„¤ì •',
      anthropicKey: process.env['ANTHROPIC_API_KEY'] ? 'ì„¤ì •ë¨' : 'ë¯¸ì„¤ì •',
      googleKey: process.env['GOOGLE_AI_API_KEY'] ? 'ì„¤ì •ë¨' : 'ë¯¸ì„¤ì •',
      requestProvider: request.provider
    });

    // ìš”ì²­ ê²€ì¦
    if (!request.provider || !request.model || !request.messages) {
      return res.status(400).json({
        success: false,
        error: 'Provider, model, and messages are required',
      });
    }

    if (!Array.isArray(request.messages) || request.messages.length === 0) {
      return res.status(400).json({
        success: false,
        error: 'Messages must be a non-empty array',
      });
    }

    // MCP ì»¨í…ìŠ¤íŠ¸ í†µí•© ì²˜ë¦¬
    let enrichedContext: EnrichedContext | null = null;
    let contextMessages = request.messages;

    if (request.useContext && request.sessionId) {
      try {
        console.log(`ğŸ§  ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ AI ë¶„ì„ ì‹œì‘: ${request.sessionId}`);

        // ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë˜ëŠ” ìºì‹œì—ì„œ ì¡°íšŒ
        enrichedContext = await contextCache.getOrUpdate(
          request.sessionId,
          request.contextOptions || {},
          request.contextOptions?.forceRefresh || false
        );

        // ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
        if (enrichedContext && request.analysisType) {
          const mappedAnalysisType = (() => {
            switch (request.analysisType) {
              case 'project_analysis': return 'project' as const;
              case 'market_research': return 'market' as const;
              case 'tech_evaluation': return 'technical' as const;
              default: return 'comprehensive' as const;
            }
          })();

          const contextAwarePrompt = promptEngine.buildContextAwarePrompt(
            request.messages[request.messages.length - 1]?.content || '',
            enrichedContext,
            mappedAnalysisType
          );

          // ê¸°ì¡´ ë©”ì‹œì§€ë¥¼ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ë¡œ êµì²´
          contextMessages = [
            {
              role: 'system' as const,
              content: contextAwarePrompt.systemPrompt
            },
            {
              role: 'user' as const,
              content: contextAwarePrompt.userPrompt
            }
          ];

          console.log(`âœ… ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (ì‹ ë¢°ë„: ${(enrichedContext.metadata.totalConfidence * 100).toFixed(1)}%)`);
        }
      } catch (error) {
        console.warn('âš ï¸ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ ì§„í–‰:', error);
        // ì»¨í…ìŠ¤íŠ¸ ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ AI í˜¸ì¶œì€ ê³„ì† ì§„í–‰
      }
    }

    // AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    const { openai, anthropic, google } = initializeClients();

    let result: AICompletionResponse;

    // ì»¨í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì²­ ê°ì²´ ìˆ˜ì •
    const enhancedRequest = {
      ...request,
      messages: contextMessages
    };

    switch (request.provider) {
      case 'openai':
        if (!openai) {
          return res.status(500).json({
            success: false,
            error: 'OpenAI API key not configured',
          });
        }
        result = await callOpenAI(openai, enhancedRequest);
        break;

      case 'anthropic':
        if (!anthropic) {
          return res.status(500).json({
            success: false,
            error: 'Anthropic API key not configured',
          });
        }
        result = await callAnthropic(anthropic, enhancedRequest);
        break;

      case 'google':
        if (!google) {
          return res.status(500).json({
            success: false,
            error: 'Google AI API key not configured',
          });
        }
        result = await callGoogle(google, enhancedRequest);
        break;

      default:
        return res.status(400).json({
          success: false,
          error: `Unsupported provider: ${request.provider}`,
        });
    }

    // ì»¨í…ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€
    if (result.success && result.data && enrichedContext) {
      result.data.contextUsed = true;
      result.data.contextMetadata = {
        sessionId: enrichedContext.sessionId,
        dataSourceCount: enrichedContext.metadata.dataSourceCount,
        totalConfidence: enrichedContext.metadata.totalConfidence,
        lastUpdated: enrichedContext.metadata.lastUpdated,
        cacheHit: request.contextOptions?.forceRefresh ? false : true
      };

      console.log(`ğŸ¯ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ AI ë¶„ì„ ì™„ë£Œ: ${enrichedContext.sessionId} (${enrichedContext.metadata.dataSourceCount}ê°œ ì†ŒìŠ¤)`);
    } else if (result.success && result.data) {
      result.data.contextUsed = false;
    }

    // ì„±ê³µ/ì‹¤íŒ¨ì— ë”°ë¥¸ ìƒíƒœ ì½”ë“œ ì„¤ì •
    const statusCode = result.success ? 200 : 500;
    return res.status(statusCode).json(result);

  } catch (error) {
    console.error('API handler error:', error);
    return res.status(500).json({
      success: false,
      error: error instanceof Error ? error.message : 'Internal server error',
    });
  }
}