// Vercel API ë¼ìš°íŠ¸ - AI ì§ˆë¬¸ ìƒì„± ì „ìš© ì—”ë“œí¬ì¸íŠ¸
// ì‚¬ì „ ë¶„ì„ ë‹¨ê³„ì—ì„œ ë¬¸ì„œ ê¸°ë°˜ ë§ì¶¤í˜• ì§ˆë¬¸ ìƒì„±ì— íŠ¹í™”

import type { VercelRequest, VercelResponse } from '@vercel/node'

interface QuestionRequest {
  provider: 'openai' | 'anthropic' | 'google'
  model: string
  projectId: string
  projectInfo: {
    name?: string
    description?: string
    industry?: string
  }
  documents: Array<{
    name: string
    summary?: string
    content?: string
  }>
  context?: {
    userId?: string
    sessionId?: string
    requestType?: string
  }
}

interface GeneratedQuestion {
  category: string
  text: string
  type: 'text' | 'select' | 'multiselect' | 'number' | 'textarea'
  options?: string[]
  required: boolean
  helpText?: string
  priority: 'high' | 'medium' | 'low'
  confidence: number
}

interface QuestionResponse {
  questions: GeneratedQuestion[]
  usage: {
    inputTokens: number
    outputTokens: number
    totalTokens: number
  }
  cost: {
    inputCost: number
    outputCost: number
    totalCost: number
  }
  model: string
  responseTime: number
  metadata: {
    projectId: string
    totalQuestions: number
    categories: string[]
  }
}

export default async function handler(
  req: VercelRequest,
  res: VercelResponse
) {
  console.log('ğŸš€ [AI Questions API] ì§ˆë¬¸ ìƒì„± ìš”ì²­ ìˆ˜ì‹ :', {
    timestamp: new Date().toISOString(),
    method: req.method,
    hasBody: !!req.body
  })

  // CORS í—¤ë” ì¶”ê°€
  res.setHeader('Access-Control-Allow-Origin', '*')
  res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS')
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type, Authorization')

  // OPTIONS ìš”ì²­ ì²˜ë¦¬
  if (req.method === 'OPTIONS') {
    return res.status(200).end()
  }

  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' })
  }

  try {
    const requestBody: QuestionRequest = req.body

    console.log('ğŸ“ [AI Questions API] ìš”ì²­ ë¶„ì„:', {
      provider: requestBody.provider,
      model: requestBody.model,
      projectId: requestBody.projectId,
      documentsCount: requestBody.documents?.length || 0,
      hasProjectInfo: !!requestBody.projectInfo,
      projectName: requestBody.projectInfo?.name
    })

    // í•„ìˆ˜ íŒŒë¼ë¯¸í„° ê²€ì¦
    if (!requestBody.provider || !requestBody.model || !requestBody.projectId) {
      console.error('âŒ [AI Questions API] í•„ìˆ˜ íŒŒë¼ë¯¸í„° ëˆ„ë½:', {
        hasProvider: !!requestBody.provider,
        hasModel: !!requestBody.model,
        hasProjectId: !!requestBody.projectId
      })
      return res.status(400).json({
        error: 'í•„ìˆ˜ íŒŒë¼ë¯¸í„°ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.',
        required: ['provider', 'model', 'projectId']
      })
    }

    // í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    const apiKeys = {
      openai: process.env['OPENAI_API_KEY'],
      anthropic: process.env['ANTHROPIC_API_KEY'],
      google: process.env['GOOGLE_AI_API_KEY']
    }

    const apiKey = apiKeys[requestBody.provider]
    if (!apiKey) {
      console.error(`âŒ [AI Questions API] ${requestBody.provider} API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.`)
      return res.status(500).json({
        error: `${requestBody.provider} API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.`,
        provider: requestBody.provider
      })
    }

    console.log(`ğŸ¤– [AI Questions API] AI ì§ˆë¬¸ ìƒì„± ì‹œì‘: ${requestBody.provider} ${requestBody.model}`)

    // AI í”„ë¡¬í”„íŠ¸ ìƒì„±
    const prompt = buildQuestionPrompt(requestBody)
    console.log('ğŸ“„ [AI Questions API] í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ, ê¸¸ì´:', prompt.length)

    // AI API í˜¸ì¶œ
    const aiResponse = await callAIForQuestions(
      requestBody.provider,
      apiKey,
      requestBody.model,
      prompt
    )

    console.log('âœ… [AI Questions API] AI ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ:', {
      contentLength: aiResponse.content?.length || 0,
      inputTokens: aiResponse.usage.inputTokens,
      outputTokens: aiResponse.usage.outputTokens
    })

    // ì‘ë‹µ íŒŒì‹± ë° ì§ˆë¬¸ ì¶”ì¶œ
    const questions = parseQuestions(aiResponse.content)
    console.log('ğŸ“Š [AI Questions API] ì§ˆë¬¸ íŒŒì‹± ì™„ë£Œ:', {
      questionsCount: questions.length,
      categories: [...new Set(questions.map(q => q.category))]
    })

    if (questions.length === 0) {
      throw new Error('AIì—ì„œ ìœ íš¨í•œ ì§ˆë¬¸ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.')
    }

    const response: QuestionResponse = {
      questions,
      usage: aiResponse.usage,
      cost: aiResponse.cost,
      model: requestBody.model,
      responseTime: aiResponse.responseTime,
      metadata: {
        projectId: requestBody.projectId,
        totalQuestions: questions.length,
        categories: [...new Set(questions.map(q => q.category))]
      }
    }

    console.log(`âœ… [AI Questions API] ì§ˆë¬¸ ìƒì„± ì™„ë£Œ: ${questions.length}ê°œ ì§ˆë¬¸, $${response.cost.totalCost.toFixed(4)}`)
    return res.status(200).json(response)

  } catch (error) {
    console.error('âŒ [AI Questions API] ì˜¤ë¥˜ ìƒì„¸:', {
      error: error instanceof Error ? error.message : String(error),
      stack: error instanceof Error ? error.stack : undefined,
      timestamp: new Date().toISOString()
    })

    return res.status(500).json({
      error: 'AI ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
      details: error instanceof Error ? error.message : String(error),
      timestamp: new Date().toISOString()
    })
  }
}

function buildQuestionPrompt(request: QuestionRequest): string {
  const { projectInfo, documents } = request

  let prompt = `ë‹¹ì‹ ì€ ì „ë¬¸ í”„ë¡œì íŠ¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ì „ ë¶„ì„ ë‹¨ê³„ì—ì„œ í•„ìš”í•œ í•µì‹¬ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

í”„ë¡œì íŠ¸ ì •ë³´:
- ì´ë¦„: ${projectInfo?.name || 'ë¯¸ì •'}
- ì„¤ëª…: ${projectInfo?.description || 'ë¯¸ì •'}
- ì‚°ì—… ë¶„ì•¼: ${projectInfo?.industry || 'ë¯¸ì •'}
`

  if (documents && documents.length > 0) {
    prompt += `\nì—…ë¡œë“œëœ ë¬¸ì„œë“¤:
${documents.map((doc, index) => `${index + 1}. ${doc.name}${doc.summary ? ` - ${doc.summary}` : ''}`).join('\n')}
`
  }

  prompt += `
ìš”êµ¬ì‚¬í•­:
1. í”„ë¡œì íŠ¸ íŠ¹ì„±ì— ë§ëŠ” 5-8ê°œì˜ í•µì‹¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.
2. ê° ì§ˆë¬¸ì€ í”„ë¡œì íŠ¸ ì´í•´ë¥¼ ë•ëŠ” ì‹¤ì§ˆì ì¸ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•œ ê²ƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.
3. ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬(ê¸°ìˆ , ë¹„ì¦ˆë‹ˆìŠ¤, ì¼ì •, ì˜ˆì‚°, ìœ„í—˜ ë“±)ë¥¼ í¬í•¨í•˜ì„¸ìš”.
4. ê° ì§ˆë¬¸ì˜ ì¤‘ìš”ë„ì™€ ì‹ ë¢°ë„ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ (JSON):
{
  "questions": [
    {
      "category": "ì¹´í…Œê³ ë¦¬ëª…",
      "text": "ì§ˆë¬¸ ë‚´ìš©",
      "type": "text|select|multiselect|number|textarea",
      "options": ["ì˜µì…˜1", "ì˜µì…˜2"] (select/multiselectì¸ ê²½ìš°),
      "required": true|false,
      "helpText": "ì§ˆë¬¸ì— ëŒ€í•œ ë„ì›€ë§",
      "priority": "high|medium|low",
      "confidence": 0.0-1.0
    }
  ]
}

ì •í™•í•œ JSON í˜•ì‹ë§Œ ë°˜í™˜í•˜ê³  ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.`

  return prompt
}

async function callAIForQuestions(
  provider: string,
  apiKey: string,
  model: string,
  prompt: string
): Promise<any> {
  const startTime = Date.now()

  let response: Response
  let requestBody: any

  switch (provider) {
    case 'anthropic':
      response = await fetch('https://api.anthropic.com/v1/messages', {
        method: 'POST',
        headers: {
          'x-api-key': apiKey,
          'Content-Type': 'application/json',
          'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify({
          model,
          max_tokens: 3000,
          temperature: 0.7,
          messages: [{ role: 'user', content: prompt }]
        })
      })
      break

    case 'openai':
      response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model,
          messages: [{ role: 'user', content: prompt }],
          max_tokens: 3000,
          temperature: 0.7
        })
      })
      break

    case 'google':
      response = await fetch(
        `https://generativelanguage.googleapis.com/v1/models/${model}:generateContent?key=${apiKey}`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({
            contents: [{ parts: [{ text: prompt }] }],
            generationConfig: {
              maxOutputTokens: 3000,
              temperature: 0.7
            }
          })
        }
      )
      break

    default:
      throw new Error(`ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡œë°”ì´ë”: ${provider}`)
  }

  if (!response.ok) {
    const errorText = await response.text()
    throw new Error(`${provider} API ì˜¤ë¥˜: ${response.status} - ${errorText}`)
  }

  const data = await response.json()
  const responseTime = Date.now() - startTime

  let content: string
  let usage: any
  let cost: any

  switch (provider) {
    case 'anthropic':
      content = data.content[0].text
      const inputTokens = estimateTokens(prompt)
      const outputTokens = estimateTokens(content)
      const pricing = getAnthropicPricing(model)
      usage = {
        inputTokens,
        outputTokens,
        totalTokens: inputTokens + outputTokens
      }
      cost = {
        inputCost: (inputTokens * pricing.inputCost) / 1000000,
        outputCost: (outputTokens * pricing.outputCost) / 1000000,
        totalCost: ((inputTokens * pricing.inputCost) + (outputTokens * pricing.outputCost)) / 1000000
      }
      break

    case 'openai':
      content = data.choices[0].message.content
      const openaiPricing = getOpenAIPricing(model)
      usage = {
        inputTokens: data.usage.prompt_tokens,
        outputTokens: data.usage.completion_tokens,
        totalTokens: data.usage.total_tokens
      }
      cost = {
        inputCost: (data.usage.prompt_tokens * openaiPricing.inputCost) / 1000000,
        outputCost: (data.usage.completion_tokens * openaiPricing.outputCost) / 1000000,
        totalCost: ((data.usage.prompt_tokens * openaiPricing.inputCost) + (data.usage.completion_tokens * openaiPricing.outputCost)) / 1000000
      }
      break

    case 'google':
      content = data.candidates[0].content.parts[0].text
      const googleInputTokens = estimateTokens(prompt)
      const googleOutputTokens = estimateTokens(content)
      const googlePricing = getGoogleAIPricing(model)
      usage = {
        inputTokens: googleInputTokens,
        outputTokens: googleOutputTokens,
        totalTokens: googleInputTokens + googleOutputTokens
      }
      cost = {
        inputCost: (googleInputTokens * googlePricing.inputCost) / 1000000,
        outputCost: (googleOutputTokens * googlePricing.outputCost) / 1000000,
        totalCost: ((googleInputTokens * googlePricing.inputCost) + (googleOutputTokens * googlePricing.outputCost)) / 1000000
      }
      break

    default:
      throw new Error(`ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡œë°”ì´ë”: ${provider}`)
  }

  return {
    content,
    usage,
    cost,
    responseTime
  }
}

function parseQuestions(response: string): GeneratedQuestion[] {
  try {
    // JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
    const jsonMatch = response.match(/\{[\s\S]*\}/)
    if (!jsonMatch) {
      throw new Error('JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
    }

    const parsed = JSON.parse(jsonMatch[0])

    if (!parsed.questions || !Array.isArray(parsed.questions)) {
      throw new Error('questions ë°°ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
    }

    return parsed.questions.map((q: any) => ({
      category: q.category || 'ê¸°íƒ€',
      text: q.text || '',
      type: q.type || 'textarea',
      options: q.options,
      required: q.required || false,
      helpText: q.helpText || '',
      priority: q.priority || 'medium',
      confidence: q.confidence || 0.8
    })).filter((q: GeneratedQuestion) => q.text.trim() !== '')

  } catch (error) {
    console.error('ì§ˆë¬¸ íŒŒì‹± ì‹¤íŒ¨:', error)

    // íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì§ˆë¬¸ ë°˜í™˜
    return [
      {
        category: 'í”„ë¡œì íŠ¸ ê°œìš”',
        text: 'ì´ í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ëª©í‘œëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?',
        type: 'textarea',
        required: true,
        helpText: 'í”„ë¡œì íŠ¸ì˜ í•µì‹¬ ëª©ì ê³¼ ê¸°ëŒ€ íš¨ê³¼ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.',
        priority: 'high',
        confidence: 0.9
      },
      {
        category: 'ê¸°ìˆ  ìš”êµ¬ì‚¬í•­',
        text: 'í”„ë¡œì íŠ¸ì— í•„ìš”í•œ ì£¼ìš” ê¸°ìˆ  ìŠ¤íƒì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?',
        type: 'textarea',
        required: true,
        helpText: 'ì‚¬ìš©í•  í”„ë¡œê·¸ë˜ë° ì–¸ì–´, í”„ë ˆì„ì›Œí¬, ë°ì´í„°ë² ì´ìŠ¤ ë“±ì„ í¬í•¨í•´ì£¼ì„¸ìš”.',
        priority: 'high',
        confidence: 0.9
      },
      {
        category: 'ì¼ì • ê´€ë¦¬',
        text: 'í”„ë¡œì íŠ¸ ì™„ë£Œ ëª©í‘œ ì‹œì ì€ ì–¸ì œì…ë‹ˆê¹Œ?',
        type: 'text',
        required: true,
        helpText: 'ì˜ˆìƒ ì™„ë£Œ ë‚ ì§œë‚˜ ê¸°ê°„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.',
        priority: 'high',
        confidence: 0.9
      }
    ]
  }
}

// í† í° ì¶”ì • í•¨ìˆ˜
function estimateTokens(text: string): number {
  return Math.ceil(text.length / 4) // 1í† í° â‰ˆ 4ê¸€ì
}

// ê°€ê²© ì •ë³´ í•¨ìˆ˜ë“¤
function getAnthropicPricing(model: string): { inputCost: number; outputCost: number } {
  const pricing: Record<string, { inputCost: number; outputCost: number }> = {
    'claude-sonnet-4-20250514': { inputCost: 3, outputCost: 15 },
    'claude-3-5-sonnet-20241022': { inputCost: 3, outputCost: 15 },
    'claude-3-opus-20240229': { inputCost: 15, outputCost: 75 },
    'claude-3-haiku-20240307': { inputCost: 0.25, outputCost: 1.25 }
  }
  return pricing[model] || { inputCost: 3, outputCost: 15 }
}

function getOpenAIPricing(model: string): { inputCost: number; outputCost: number } {
  const pricing: Record<string, { inputCost: number; outputCost: number }> = {
    'gpt-4o': { inputCost: 5, outputCost: 15 },
    'gpt-4o-mini': { inputCost: 0.15, outputCost: 0.6 },
    'gpt-4-turbo': { inputCost: 10, outputCost: 30 },
    'gpt-3.5-turbo': { inputCost: 0.5, outputCost: 1.5 }
  }
  return pricing[model] || { inputCost: 5, outputCost: 15 }
}

function getGoogleAIPricing(model: string): { inputCost: number; outputCost: number } {
  const pricing: Record<string, { inputCost: number; outputCost: number }> = {
    'gemini-2.0-flash-exp': { inputCost: 0.075, outputCost: 0.3 },
    'gemini-1.5-pro': { inputCost: 1.25, outputCost: 5 },
    'gemini-1.5-flash': { inputCost: 0.075, outputCost: 0.3 }
  }
  return pricing[model] || { inputCost: 1.25, outputCost: 5 }
}