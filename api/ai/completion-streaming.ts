// Vercel API ÎùºÏö∞Ìä∏ - AI Ïä§Ìä∏Î¶¨Î∞ç ÏôÑÏÑ± ÏöîÏ≤≠ Ï≤òÎ¶¨ (Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ï†ÑÏö©)
// ÏùºÎ∞ò completion.tsÎäî ÏßàÎ¨∏ ÏÉùÏÑ±Ïö©ÏúºÎ°ú Ïú†ÏßÄÌïòÍ≥†, Ïù¥ ÌååÏùºÏùÄ Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ï†ÑÏö© Ïä§Ìä∏Î¶¨Î∞ç API

import type { VercelRequest, VercelResponse } from '@vercel/node'

interface CompletionStreamRequest {
  provider: 'openai' | 'anthropic' | 'google'
  model: string
  prompt: string
  maxTokens?: number
  temperature?: number
  topP?: number
}

// Vercel ÏÑúÎ≤ÑÎ¶¨Ïä§ Ìï®Ïàò ÏÑ§Ï†ï
// Ïä§Ìä∏Î¶¨Î∞ç Î∞©ÏãùÏùÄ Ï≤´ ÏùëÎãµÎßå 60Ï¥à ÏïàÏóê ÏãúÏûëÌïòÎ©¥ Ïù¥ÌõÑ Î¨¥Ï†úÌïú
export const config = {
  maxDuration: 60, // Pro ÌîåÎûú ÏµúÎåÄÍ∞í (Ï≤´ ÏùëÎãµÎßå Îπ†Î•¥Í≤å ÏãúÏûëÌïòÎ©¥ Îê®)
}

export default async function handler(
  req: VercelRequest,
  res: VercelResponse
) {
  // CORS Ìó§Îçî Ï∂îÍ∞Ä
  res.setHeader('Access-Control-Allow-Origin', '*')
  res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS')
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type, Authorization')

  // OPTIONS ÏöîÏ≤≠ Ï≤òÎ¶¨
  if (req.method === 'OPTIONS') {
    return res.status(200).end()
  }

  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' })
  }

  try {
    console.log('üåä [Streaming API] Ïä§Ìä∏Î¶¨Î∞ç ÏöîÏ≤≠ ÏàòÏã†:', {
      timestamp: new Date().toISOString(),
      userAgent: req.headers['user-agent'],
    })

    const { provider, model, prompt, maxTokens, temperature, topP }: CompletionStreamRequest = req.body

    console.log('üìù [Streaming API] ÏöîÏ≤≠ ÌååÎùºÎØ∏ÌÑ∞:', {
      provider,
      model,
      promptLength: prompt?.length || 0,
      maxTokens,
      temperature
    })

    if (!provider || !model || !prompt) {
      console.error('‚ùå [Streaming API] ÌïÑÏàò ÌååÎùºÎØ∏ÌÑ∞ ÎàÑÎùΩ:', { provider, model, hasPrompt: !!prompt })
      return res.status(400).json({ error: 'Missing required parameters' })
    }

    // ÌôòÍ≤Ω Î≥ÄÏàòÏóêÏÑú API ÌÇ§ Í∞ÄÏ†∏Ïò§Í∏∞
    const apiKeys = {
      openai: process.env['OPENAI_API_KEY'],
      anthropic: process.env['ANTHROPIC_API_KEY'],
      google: process.env['GOOGLE_AI_API_KEY']
    }

    const apiKey = apiKeys[provider]
    if (!apiKey) {
      console.error(`‚ùå [Streaming API] ${provider} API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.`)
      return res.status(500).json({
        error: `${provider} API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.`,
        provider,
      })
    }

    // ‚úÖ SSE (Server-Sent Events) Ìó§Îçî ÏÑ§Ï†ï
    res.setHeader('Content-Type', 'text/event-stream')
    res.setHeader('Cache-Control', 'no-cache, no-transform')
    res.setHeader('Connection', 'keep-alive')
    res.setHeader('X-Accel-Buffering', 'no') // Nginx Î≤ÑÌçºÎßÅ Î∞©ÏßÄ

    console.log(`üöÄ [Streaming API] Ïä§Ìä∏Î¶¨Î∞ç ÏãúÏûë: ${provider} ${model}`)

    // ProviderÎ≥Ñ Ïä§Ìä∏Î¶¨Î∞ç Ï≤òÎ¶¨
    switch (provider) {
      case 'anthropic':
        await handleAnthropicStreaming(res, apiKey, model, prompt, maxTokens, temperature, topP)
        break
      case 'openai':
        await handleOpenAIStreaming(res, apiKey, model, prompt, maxTokens, temperature, topP)
        break
      case 'google':
        await handleGoogleAIStreaming(res, apiKey, model, prompt, maxTokens, temperature, topP)
        break
      default:
        res.write(`data: ${JSON.stringify({ type: 'error', error: `ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌîÑÎ°úÎ∞îÏù¥Îçî: ${provider}` })}\n\n`)
        res.end()
        return
    }

  } catch (error) {
    console.error('‚ùå [Streaming API] Ïä§Ìä∏Î¶¨Î∞ç Ïò§Î•ò:', {
      error: error instanceof Error ? error.message : String(error),
      stack: error instanceof Error ? error.stack : undefined,
    })

    // Ïä§Ìä∏Î¶¨Î∞ç ÎèÑÏ§ë ÏóêÎü¨ Î∞úÏÉù Ïãú
    try {
      res.write(`data: ${JSON.stringify({
        type: 'error',
        error: error instanceof Error ? error.message : 'Ïä§Ìä∏Î¶¨Î∞ç Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.'
      })}\n\n`)
      res.end()
    } catch (writeError) {
      // Ïù¥ÎØ∏ Ï¢ÖÎ£åÎêú Ïó∞Í≤∞Ïùº Ïàò ÏûàÏùå
      console.error('ÏùëÎãµ Ïì∞Í∏∞ Ïã§Ìå®:', writeError)
    }
  }
}

// Anthropic Ïä§Ìä∏Î¶¨Î∞ç Ï≤òÎ¶¨
async function handleAnthropicStreaming(
  res: VercelResponse,
  apiKey: string,
  model: string,
  prompt: string,
  maxTokens = 3000,
  temperature = 0.3,
  topP = 1
) {
  const startTime = Date.now()

  console.log('ü§ñ [Anthropic Stream] Ïä§Ìä∏Î¶¨Î∞ç ÏöîÏ≤≠ ÏãúÏûë')

  const response = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: {
      'x-api-key': apiKey,
      'Content-Type': 'application/json',
      'anthropic-version': '2023-06-01'
    },
    body: JSON.stringify({
      model,
      max_tokens: maxTokens,
      temperature,
      top_p: topP,
      messages: [{ role: 'user', content: prompt }],
      stream: true, // üî• Ïä§Ìä∏Î¶¨Î∞ç ÌôúÏÑ±Ìôî
    })
  })

  if (!response.ok) {
    const errorText = await response.text()
    throw new Error(`Anthropic API Ïò§Î•ò: ${response.status} - ${errorText}`)
  }

  if (!response.body) {
    throw new Error('ÏùëÎãµ Î≥∏Î¨∏Ïù¥ ÏóÜÏäµÎãàÎã§.')
  }

  const reader = response.body.getReader()
  const decoder = new TextDecoder()

  let fullContent = ''
  let inputTokens = 0
  let outputTokens = 0
  let buffer = ''

  console.log('üì• [Anthropic Stream] Ïä§Ìä∏Î¶º ÏàòÏã† ÏãúÏûë')

  try {
    while (true) {
      const { done, value } = await reader.read()

      if (done) {
        console.log('‚úÖ [Anthropic Stream] Ïä§Ìä∏Î¶º ÏôÑÎ£å')
        break
      }

      // SSE Îç∞Ïù¥ÌÑ∞ ÌååÏã±
      buffer += decoder.decode(value, { stream: true })
      const lines = buffer.split('\n')

      // ÎßàÏßÄÎßâ Î∂àÏôÑÏ†ÑÌïú ÎùºÏù∏ÏùÄ Îã§Ïùå Ï≤≠ÌÅ¨Î°ú
      buffer = lines.pop() || ''

      for (const line of lines) {
        if (line.startsWith('data:')) {
          const data = line.slice(5).trim()

          if (data === '[DONE]') continue

          try {
            const event = JSON.parse(data)

            // content_block_delta: ÌÖçÏä§Ìä∏ Ï°∞Í∞Å
            if (event.type === 'content_block_delta' && event.delta?.text) {
              fullContent += event.delta.text

              // ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î°ú Ï¶âÏãú Ï†ÑÏÜ°
              res.write(`data: ${JSON.stringify({
                type: 'text',
                content: event.delta.text,
                fullContent: fullContent
              })}\n\n`)
            }

            // message_delta: ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ Ï†ïÎ≥¥
            if (event.type === 'message_delta' && event.usage) {
              outputTokens = event.usage.output_tokens || 0
            }

            // message_start: ÏûÖÎ†• ÌÜ†ÌÅ∞ Ï†ïÎ≥¥
            if (event.type === 'message_start' && event.message?.usage) {
              inputTokens = event.message.usage.input_tokens || 0
            }

          } catch (parseError) {
            console.warn('‚ö†Ô∏è SSE ÌååÏã± Ïò§Î•ò:', data)
          }
        }
      }
    }

    const responseTime = Date.now() - startTime

    // üî• Î™®Îç∏Î≥Ñ ÎπÑÏö© Í≥ÑÏÇ∞
    const pricing = getAnthropicPricing(model)
    const inputCost = (inputTokens * pricing.inputCost) / 1000000
    const outputCost = (outputTokens * pricing.outputCost) / 1000000

    // ÏµúÏ¢Ö ÏôÑÎ£å Ïù¥Î≤§Ìä∏ Îç∞Ïù¥ÌÑ∞
    const doneEvent = JSON.stringify({
      type: 'done',
      content: fullContent,
      usage: {
        inputTokens,
        outputTokens,
        totalTokens: inputTokens + outputTokens
      },
      cost: {
        inputCost,
        outputCost,
        totalCost: inputCost + outputCost
      },
      model,
      finishReason: 'stop',
      responseTime
    })

    // üî• ÏµúÏ¢Ö Ïù¥Î≤§Ìä∏Î•º Îëê Î≤à Ï†ÑÏÜ°ÌïòÏó¨ ÌôïÏã§Ìûà ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä Î∞õÎèÑÎ°ù Ìï®
    res.write(`data: ${doneEvent}\n\n`)
    // ÏïΩÍ∞ÑÏùò ÎîúÎ†àÏù¥ ÌõÑ Îã§Ïãú Ï†ÑÏÜ°
    await new Promise(resolve => setTimeout(resolve, 100))
    res.write(`data: ${doneEvent}\n\n`)

    console.log(`‚úÖ [Anthropic Stream] ÏôÑÎ£å: ${inputTokens + outputTokens} ÌÜ†ÌÅ∞, ${responseTime}ms`)

    // Ï∂îÍ∞Ä ÎîúÎ†àÏù¥ ÌõÑ Ïó∞Í≤∞ Ï¢ÖÎ£å
    await new Promise(resolve => setTimeout(resolve, 100))
    res.end()

  } catch (error) {
    console.error('‚ùå [Anthropic Stream] Ïä§Ìä∏Î¶º Ï≤òÎ¶¨ Ïò§Î•ò:', error)
    throw error
  }
}

// OpenAI Ïä§Ìä∏Î¶¨Î∞ç Ï≤òÎ¶¨
async function handleOpenAIStreaming(
  res: VercelResponse,
  apiKey: string,
  model: string,
  prompt: string,
  maxTokens = 3000,
  temperature = 0.3,
  topP = 1
) {
  const startTime = Date.now()

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model,
      messages: [{ role: 'user', content: prompt }],
      max_tokens: maxTokens,
      temperature,
      top_p: topP,
      stream: true,
    })
  })

  if (!response.ok) {
    const errorText = await response.text()
    throw new Error(`OpenAI API Ïò§Î•ò: ${response.status} - ${errorText}`)
  }

  if (!response.body) {
    throw new Error('ÏùëÎãµ Î≥∏Î¨∏Ïù¥ ÏóÜÏäµÎãàÎã§.')
  }

  const reader = response.body.getReader()
  const decoder = new TextDecoder()

  let fullContent = ''
  let inputTokens = 0
  let outputTokens = 0
  let buffer = ''

  try {
    while (true) {
      const { done, value } = await reader.read()

      if (done) break

      buffer += decoder.decode(value, { stream: true })
      const lines = buffer.split('\n')
      buffer = lines.pop() || ''

      for (const line of lines) {
        if (line.startsWith('data:')) {
          const data = line.slice(5).trim()

          if (data === '[DONE]') continue

          try {
            const event = JSON.parse(data)
            const content = event.choices?.[0]?.delta?.content

            if (content) {
              fullContent += content

              res.write(`data: ${JSON.stringify({
                type: 'text',
                content,
                fullContent
              })}\n\n`)
            }

            // OpenAIÎäî Ïä§Ìä∏Î¶¨Î∞çÏóêÏÑú ÌÜ†ÌÅ∞ Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌïòÏßÄ ÏïäÏúºÎØÄÎ°ú Ï∂îÏ†ï
            if (event.usage) {
              inputTokens = event.usage.prompt_tokens
              outputTokens = event.usage.completion_tokens
            }

          } catch (parseError) {
            console.warn('‚ö†Ô∏è SSE ÌååÏã± Ïò§Î•ò:', data)
          }
        }
      }
    }

    // ÌÜ†ÌÅ∞Ïù¥ ÏóÜÏúºÎ©¥ Ï∂îÏ†ï
    if (!inputTokens) inputTokens = estimateTokens(prompt, 'openai')
    if (!outputTokens) outputTokens = estimateTokens(fullContent, 'openai')

    const responseTime = Date.now() - startTime
    const pricing = getOpenAIPricing(model)
    const inputCost = (inputTokens * pricing.inputCost) / 1000000
    const outputCost = (outputTokens * pricing.outputCost) / 1000000

    // ÏµúÏ¢Ö ÏôÑÎ£å Ïù¥Î≤§Ìä∏ Îç∞Ïù¥ÌÑ∞
    const doneEvent = JSON.stringify({
      type: 'done',
      content: fullContent,
      usage: { inputTokens, outputTokens, totalTokens: inputTokens + outputTokens },
      cost: { inputCost, outputCost, totalCost: inputCost + outputCost },
      model,
      finishReason: 'stop',
      responseTime
    })

    // üî• ÏµúÏ¢Ö Ïù¥Î≤§Ìä∏Î•º Îëê Î≤à Ï†ÑÏÜ°ÌïòÏó¨ ÌôïÏã§Ìûà ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä Î∞õÎèÑÎ°ù Ìï®
    res.write(`data: ${doneEvent}\n\n`)
    await new Promise(resolve => setTimeout(resolve, 100))
    res.write(`data: ${doneEvent}\n\n`)

    console.log(`‚úÖ [OpenAI Stream] ÏôÑÎ£å: ${inputTokens + outputTokens} ÌÜ†ÌÅ∞, ${responseTime}ms`)

    // Ï∂îÍ∞Ä ÎîúÎ†àÏù¥ ÌõÑ Ïó∞Í≤∞ Ï¢ÖÎ£å
    await new Promise(resolve => setTimeout(resolve, 100))
    res.end()

  } catch (error) {
    console.error('‚ùå [OpenAI Stream] Ïä§Ìä∏Î¶º Ï≤òÎ¶¨ Ïò§Î•ò:', error)
    throw error
  }
}

// Google AI Ïä§Ìä∏Î¶¨Î∞ç Ï≤òÎ¶¨
async function handleGoogleAIStreaming(
  res: VercelResponse,
  apiKey: string,
  model: string,
  prompt: string,
  maxTokens = 3000,
  temperature = 0.3,
  topP = 1
) {
  const startTime = Date.now()

  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1/models/${model}:streamGenerateContent?key=${apiKey}&alt=sse`,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        contents: [{ parts: [{ text: prompt }] }],
        generationConfig: {
          maxOutputTokens: maxTokens,
          temperature,
          topP
        }
      })
    }
  )

  if (!response.ok) {
    const errorText = await response.text()
    throw new Error(`Google AI API Ïò§Î•ò: ${response.status} - ${errorText}`)
  }

  if (!response.body) {
    throw new Error('ÏùëÎãµ Î≥∏Î¨∏Ïù¥ ÏóÜÏäµÎãàÎã§.')
  }

  const reader = response.body.getReader()
  const decoder = new TextDecoder()

  let fullContent = ''
  let buffer = ''

  try {
    while (true) {
      const { done, value } = await reader.read()

      if (done) break

      buffer += decoder.decode(value, { stream: true })
      const lines = buffer.split('\n')
      buffer = lines.pop() || ''

      for (const line of lines) {
        if (line.startsWith('data:')) {
          const data = line.slice(5).trim()

          try {
            const event = JSON.parse(data)
            const content = event.candidates?.[0]?.content?.parts?.[0]?.text

            if (content) {
              fullContent += content

              res.write(`data: ${JSON.stringify({
                type: 'text',
                content,
                fullContent
              })}\n\n`)
            }

          } catch (parseError) {
            console.warn('‚ö†Ô∏è SSE ÌååÏã± Ïò§Î•ò:', data)
          }
        }
      }
    }

    const inputTokens = estimateTokens(prompt, 'google')
    const outputTokens = estimateTokens(fullContent, 'google')
    const responseTime = Date.now() - startTime
    const pricing = getGoogleAIPricing(model)
    const inputCost = (inputTokens * pricing.inputCost) / 1000000
    const outputCost = (outputTokens * pricing.outputCost) / 1000000

    // ÏµúÏ¢Ö ÏôÑÎ£å Ïù¥Î≤§Ìä∏ Îç∞Ïù¥ÌÑ∞
    const doneEvent = JSON.stringify({
      type: 'done',
      content: fullContent,
      usage: { inputTokens, outputTokens, totalTokens: inputTokens + outputTokens },
      cost: { inputCost, outputCost, totalCost: inputCost + outputCost },
      model,
      finishReason: 'stop',
      responseTime
    })

    // üî• ÏµúÏ¢Ö Ïù¥Î≤§Ìä∏Î•º Îëê Î≤à Ï†ÑÏÜ°ÌïòÏó¨ ÌôïÏã§Ìûà ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä Î∞õÎèÑÎ°ù Ìï®
    res.write(`data: ${doneEvent}\n\n`)
    await new Promise(resolve => setTimeout(resolve, 100))
    res.write(`data: ${doneEvent}\n\n`)

    console.log(`‚úÖ [Google AI Stream] ÏôÑÎ£å: ${inputTokens + outputTokens} ÌÜ†ÌÅ∞, ${responseTime}ms`)

    // Ï∂îÍ∞Ä ÎîúÎ†àÏù¥ ÌõÑ Ïó∞Í≤∞ Ï¢ÖÎ£å
    await new Promise(resolve => setTimeout(resolve, 100))
    res.end()

  } catch (error) {
    console.error('‚ùå [Google AI Stream] Ïä§Ìä∏Î¶º Ï≤òÎ¶¨ Ïò§Î•ò:', error)
    throw error
  }
}

// ÌÜ†ÌÅ∞ Ï∂îÏ†ï Ìï®Ïàò
function estimateTokens(text: string, provider: string): number {
  const length = text.length
  switch (provider) {
    case 'anthropic':
      return Math.ceil(length / 3.5)
    case 'openai':
      return Math.ceil(length / 4)
    case 'google':
      return Math.ceil(length / 4)
    default:
      return Math.ceil(length / 4)
  }
}

// Í∞ÄÍ≤© Ï†ïÎ≥¥ Ìï®ÏàòÎì§
function getAnthropicPricing(model: string): { inputCost: number; outputCost: number } {
  const pricing: Record<string, { inputCost: number; outputCost: number }> = {
    'claude-sonnet-4-20250514': { inputCost: 3, outputCost: 15 },
    'claude-3-5-sonnet-20241022': { inputCost: 3, outputCost: 15 },
    'claude-3-opus-20240229': { inputCost: 15, outputCost: 75 },
    'claude-3-haiku-20240307': { inputCost: 0.25, outputCost: 1.25 }
  }
  return pricing[model] || { inputCost: 3, outputCost: 15 }
}

function getOpenAIPricing(model: string): { inputCost: number; outputCost: number } {
  const pricing: Record<string, { inputCost: number; outputCost: number }> = {
    'gpt-4o': { inputCost: 5, outputCost: 15 },
    'gpt-4o-mini': { inputCost: 0.15, outputCost: 0.6 },
    'gpt-4-turbo': { inputCost: 10, outputCost: 30 },
    'gpt-3.5-turbo': { inputCost: 0.5, outputCost: 1.5 }
  }
  return pricing[model] || { inputCost: 5, outputCost: 15 }
}

function getGoogleAIPricing(model: string): { inputCost: number; outputCost: number } {
  const pricing: Record<string, { inputCost: number; outputCost: number }> = {
    'gemini-2.0-flash-exp': { inputCost: 0.075, outputCost: 0.3 },
    'gemini-1.5-pro': { inputCost: 1.25, outputCost: 5 },
    'gemini-1.5-flash': { inputCost: 0.075, outputCost: 0.3 }
  }
  return pricing[model] || { inputCost: 1.25, outputCost: 5 }
}
